# -*- coding: utf-8 -*-
"""Copy of extractweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o6OODzhB2C4BWGqLMU4iYDVBJJsKqucm

## Extract Tweets

# 1.- Librarys
"""


import os
from dotenv import load_dotenv
import requests
import tweepy
import datetime as dt
import pandas as pd
import numpy as np
import csv

import warnings
warnings.filterwarnings('ignore')

"""# 2.- Credentials"""

# Read key-value pairs from a .env file and set them as environment variables.

print(load_dotenv('keysTwe.env', override = True))
#Here we will pass the credentials of 2 developer accounts
#  that we obtained in
#case any of them were banned.

#Source 1

twitter_key = os.environ.get('api_key')
twitter_secret_key = os.environ.get('secret_key')
bearer_token = os.environ.get('bearer_token')

#Source 2
twitter_key1 = os.environ.get('api_key1')
twitter_secret_key1 = os.environ.get('secret_key1')
bearer_token1 = os.environ.get('bearer_token1')

"""### 3.1.1 Search Tweet Function"""

#request Tweets according to a specific query.
#bearer_token: Security token from Twitter API
#next_token: ID of the next page that matches the specified query

def search_tweets(query, bearer_token = bearer_token, next_token = None):


  headers = {"Authorization": "Bearer {}".format(bearer_token)}
    
  # end point
  url = f"https://api.twitter.com/2/tweets/search/recent?query={query}&"

  params = {
      # select specific Tweets fields from each returned Tweet object
      # public_metrics
      'tweet.fields': 'text,created_at,lang,possibly_sensitive,geo,source',
        
      # maximum number of search results(100)
      'max_results': 100,
        
      # additional data that relate to the originally returned Tweets
      'expansions': 'author_id,referenced_tweets.id,geo.place_id',
         
      # Places 
      "place.fields": 'country,full_name,name',
        
      # user fields
      "user.fields": 'location,verified,username' ,
        
      # get to next page of results.
      "next_token": next_token,
      }
      
  # Request to information
  response = requests.get(url = url, params = params, headers = headers)

  # If the request is ok, verified successfull request
  if response.status_code != 200:
    raise Exception(response.status_code, response.text)
  else:
    return response.json()

# query: string that will be used to find tweets
query = 'seleccionmexicana'

# search term
search_tweet = search_tweets(query = query)

# 4 main keys
search_tweet.keys()

search_tweet['includes'].keys()

"""### 3.1.2  Preprocessing"""

# Get current dateand later to the names of the files generated.
today = dt.date.today()
today = today.strftime("%Y-%m-%d")
today

def create_dataframes(json_tweets, today):
  
  if "places" in json_tweets['includes'].keys():
    
    # If the field exists, create a dataframe with the corresponding data
    places = pd.json_normalize(json_tweets['includes']['places']).rename(columns = {"id":"geo.place_id"})
        
    # Create users dataframe
    users = pd.json_normalize(json_tweets['includes']['users']).rename(columns = {"id":"user_id"})
    
    # Create df with tweet's data
    tweets = pd.json_normalize(json_tweets['data']).rename(columns = {"id":"tweet_id"})
        
    # Get tweet's type
    tweets['type'] = tweets.referenced_tweets.apply(lambda x: x[0]["type"] if type(x) == list else None)
        
    # Drop retweeted tweets and tweets with undefined Language
    tweets = tweets[tweets["type"] != "retweeted"].reset_index(drop = True)
    tweets = tweets[tweets["lang"] != "und"]
    
    # id to string
    tweets["tweet_id"] = tweets["tweet_id"].astype(str)
    
    # List of users in tweets dataframe to only 
    # keep users from tweets dataframe
        
    user_list = tweets.author_id.unique()
    users = users.loc[users.user_id.isin(user_list)].reset_index(drop = True)
        
    # id to string
    users["user_id"] = users["user_id"].astype(str)
        
    # from string to datetime
    tweets["created_at"] = pd.to_datetime(tweets["created_at"], utc = True)
        
    # Drop cols
    tweets = tweets.drop(['referenced_tweets','author_id','geo.place_id'], axis = 1)
    return tweets, users, places
    
    # Only return users and tweets dataframes since any tweet 
    # contained information about the place where it was tweeted.
  else: 
    # Create users dataframe
    users = pd.json_normalize(json_tweets['includes']['users']).rename(columns = {"id":"user_id"})
    
    # Create df with tweet's data
    tweets = pd.json_normalize(json_tweets['data']).rename(columns = {"id":"tweet_id"})
        
    # Get tweet's type
    tweets['type'] = tweets.referenced_tweets.apply(lambda x: x[0]["type"] if type(x) == list else None)
        
    # Drop retweeted tweets
    tweets = tweets[tweets["type"] != "retweeted"].reset_index(drop = True)
        
    # id to string
    tweets["tweet_id"] = tweets["tweet_id"].astype(str)
        
    # List of users in tweets dataframe
    user_list = tweets.author_id.unique()

    # Only keep users from tweets dataframe
    users = users.loc[users.user_id.isin(user_list)].reset_index(drop = True)
        
    # id to string
    users["user_id"] = users["user_id"].astype(str)

## Check if we have tweet's location
#if "places" in search_tweet['includes'].keys():
#  main_tweets, main_users, main_places = create_dataframes(search_tweet, today)
    

#else:
  #main_tweets, main_users = create_dataframes(search_tweet, today)
#  main_places = pd.DataFrame()


def generate_map_metrics():

    twitter_key = os.environ.get('api_key')
    twitter_secret_key = os.environ.get('secret_key')
    bearer_token = os.environ.get('bearer_token')

  #Source 2
    twitter_key1 = os.environ.get('api_key1')
    twitter_secret_key1 = os.environ.get('secret_key1')
    bearer_token1 = os.environ.get('bearer_token1')

    #query: string that will be used to find tweets
    query = 'mexico'
     # search term
    search_tweet = search_tweets(query = query)
    # 4 main keys
    search_tweet.keys()
    
    # Get current dateand later to the names of the files generated.
    today = dt.date.today()
    today = today.strftime("%Y-%m-%d")
    #today
    ## Check if we have tweet's location
    df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_world_gdp_with_codes.csv')
    cnt = list(df['CODE'])
    #cnt_t = list(df33['country_code'])
    #count = list(df33['name'])
    new = [0  for i in range(len(cnt))]
    
    
    if "places" in search_tweet['includes'].keys():
        main_tweets, main_users, main_places = create_dataframes(search_tweet, today)
        df33 = main_places.groupby(['country_code'],as_index=False).count()
        cnt_t = list(df33['country_code'])
        count = list(df33['name'])
        new = [0  for i in range(len(cnt))]
        for i in range(len(cnt)):
            for j in range(len(cnt_t)):
                if cnt[i] == count[j]:
                    new[i] = cnt[i]
        df['t_c'] = new

    
    else:
    #main_tweets, main_users = create_dataframes(search_tweet, today)
        main_places = pd.DataFrame()
            
    #new = [0  for i in range(len(cnt))]
    df['t_c'] = new
    return df




























