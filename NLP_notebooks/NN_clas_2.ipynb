{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b025146-51b8-458a-8b0a-bab17978631a",
   "metadata": {},
   "source": [
    "# Predictor Class \n",
    "\n",
    "## Introduction \n",
    "\n",
    "So far in this notebook we are going to create a predictor class for \n",
    "detectin to what class a given tweet belongs to. \n",
    "\n",
    "the class 1 is Mexico and the class 2 is argentina. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c4f7800b-a15b-4a50-b88a-6af48219642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class robot_class_predictor: \n",
    "    def __init__(self):\n",
    "        #here we have the global variables.so far we need \n",
    "        #1. the data frames \n",
    "        # the model architecture   \n",
    "        #Noise removal, stop word removal, normalizing?\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        self.df_class_1=None\n",
    "        self.df_class_2=None\n",
    "        self.concat_df=None\n",
    "        self.tfidf=None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def cleanString(self,s, special_chars = \"\\\":,.@|√∞√ø≈ì≈æ√∞√ø√¢≈ì≈ì√Ø√ø≈ì≈æ√ø¬∫√ø√ø≈ì≈æ√ø\"):\n",
    "        from nltk.tokenize import TweetTokenizer\n",
    "        from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "        for char in special_chars:\n",
    "            s = s.replace(char, \"\")\n",
    "        s = s.replace(\"\\n\", \"\")\n",
    "        s = s.replace(\"https\", \"\")\n",
    "        s = self.scrub_words(s)\n",
    "        tokenizer = TweetTokenizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        cleaned_words = [w for w in tokenizer.tokenize(s) if w not in stop_words]\n",
    "        return \" \".join(cleaned_words)\n",
    "    \n",
    "    def cleanFrame(self,frame):\n",
    "        frame['clean_tweet'] = frame.tweet.apply(self.cleanString)\n",
    "        #methods for cleaning the tweets\n",
    "        \n",
    "        \n",
    "    #preprocessing methods\n",
    "    def scrub_words(self,text):\n",
    "        # remove html markup\n",
    "        import re\n",
    "        text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "        #remove non-ascii and digits\n",
    "        text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "  \n",
    "        #remove whitespace\n",
    "        text=text.strip()\n",
    "        return text\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    #The methods avaialabe to Load the information  \n",
    "    def load_dataset_class_1(self):\n",
    "        df_mexico = pd.read_csv('data/all_Tweets_2022-11-28(Mexico).csv')\n",
    "        lnguage = 'en'\n",
    "        df_mexico_1 = df_mexico.loc[df_mexico['lang']==lnguage]        \n",
    "        t_1=pd.DataFrame({'tweet':list(df_mexico_1['text']),'class':0})\n",
    "        #load the data to the class\n",
    "        self.cleanFrame(t_1)\n",
    "        self.df_class_1 = t_1\n",
    "        t_1.to_csv('clean_data_c_1.csv')\n",
    "        print('Data loaded successfully')\n",
    "\n",
    "    def load_dataset_class_2(self):\n",
    "        df_argentina = pd.read_csv('data/all_Tweets_2022-11-28(argentina).csv')\n",
    "        lnguage = 'en'\n",
    "        df_argentina_1 = df_argentina.loc[df_argentina['lang']==lnguage]        \n",
    "        t_1=pd.DataFrame({'tweet':list(df_argentina_1['text']),'class':1})\n",
    "        #load the data to the class\n",
    "        self.cleanFrame(t_1)\n",
    "        self.df_class_2 = t_1\n",
    "        t_1.to_csv('clean_data_c_2.csv')\n",
    "\n",
    "        print('Data loaded successfully')\n",
    "        \n",
    "        \n",
    "    #=====================================================#\n",
    "    def tweetToVec(self,tweet, row=0):\n",
    "        words = tweet.split(\" \")\n",
    "        vec = np.zeros(self.tfidf.shape[1])\n",
    "        for w in words:\n",
    "            #print(\"including word \" + w)\n",
    "            if w in self.tfidf.columns:\n",
    "                index = self.tfidf.columns.get_loc(w)\n",
    "            #print(index, tfidf[w][row])\n",
    "            vec[index] = self.tfidf[w][row]\n",
    "        return vec\n",
    "    \n",
    "    \n",
    "    def prepare_metadata(self):\n",
    "        self.load_dataset_class_1()\n",
    "        self.load_dataset_class_2()\n",
    "        \n",
    "        self.concat_df = pd.concat([self.df_class_1,self.df_class_2])\n",
    "        \n",
    "            #from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        vectors = self.vectorizer.fit_transform(self.concat_df.tweet.tolist())\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        self.tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "        self.tfidf.head()\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # code for the training model \n",
    "    def prepare_data_for_tidf(self):\n",
    "        \n",
    "        self.load_dataset_class_1()\n",
    "        self.load_dataset_class_2()\n",
    "        \n",
    "        self.concat_df = pd.concat([self.df_class_1,self.df_class_2])\n",
    "        \n",
    "            #from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        vectors = self.vectorizer.fit_transform(self.concat_df.tweet.tolist())\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        self.tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "        self.tfidf.head()\n",
    "        \n",
    "        #print(self.tweetToVec(list(self.concat_df['tweet'])[0],self.tfidf))\n",
    "        \n",
    "        \n",
    "        X = self.tfidf.values\n",
    "        print(X.shape)\n",
    "        print(np.ones(X.shape[0]).shape)\n",
    "        #Adding bias term\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        print(X.shape)\n",
    "        \n",
    "        Y = np.array(list(map(lambda x: min(x, 1),self.concat_df['class'])))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        print(X.shape, X_train.shape, X_test.shape, len(y_train), len(y_test))\n",
    "        \n",
    "        \n",
    "        training_data = X_train\n",
    "        target_data = np.array(y_train)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(X.shape[1], activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "        model.fit(training_data, target_data, epochs=150)\n",
    " \n",
    "        scores = model.evaluate(training_data, target_data)\n",
    "\n",
    "        model.save('Training_clasification_v1.h5')\n",
    "\n",
    " \n",
    "        print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        print (model.predict(training_data).round())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ############################3\n",
    "    def predict_new(self,tweet):\n",
    "        model = tf.keras.models.load_model('Training_clasification_v1.h5')\n",
    "        #vectorize = self.tweetToVec(tweet,row=0)\n",
    "        tweet2 = [tweet,'a']\n",
    "        vectors = self.vectorizer.transform(tweet2)\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "        X=tfidf.values\n",
    "        \n",
    "        #X = vectorize\n",
    "        print(X.shape)\n",
    "        print(np.ones(X.shape[0]).shape)\n",
    "        #Adding bias term\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        print(X.shape)\n",
    "\n",
    "        val = model.predict(X).round()\n",
    "        print(model.predict(X).round())\n",
    "\n",
    "        return val\n",
    "\n",
    "            \n",
    "                                                     \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6b43d7e4-9f39-4c4c-9881-b095d1ed2ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/augusto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing.label import MultiLabelBinarizer\n",
    "from sklearn.preprocessing._label import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np \n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6eb8240a-e56a-492f-871b-d6d476f37d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_e = robot_class_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d79fef88-629d-48d7-92a1-4d4c379a909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Data loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/augusto/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "wall_e.prepare_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8ac4988-b345-46ae-a671-3d3ac5c86ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üìäN√∫meros de la #SeleccionMexicana üá≤üáΩ en Mundia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The joy of seeing your hero perform at the #FI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The joy of seeing your hero perform at the #FI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@miseleccionmxEN @miseleccionmx \\n$8.8 Million...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Playing cricket in parkikg lot. LilLoquito fou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Okay last football tweet: Mexico‚Äôs football fe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NO PUNCH! üò∞\\n\\nZero goals for Mexico in Qatar ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Well Mexico never fails to disappoint me at th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The same energy \\n\\n#SeleccionMexicana #QatarW...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mexico couldn't convert when they were the bet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mexico national team coach wanted his team to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mexico national team coach wanted his team to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How the fuck do we go in to a WC with an Argen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PTM 2do Gol de #Argentina üá¶üá∑ alv 2Ô∏è‚É£‚ûñ0Ô∏è‚É£! Pobr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  senti\n",
       "0   üìäN√∫meros de la #SeleccionMexicana üá≤üáΩ en Mundia...      0\n",
       "1   The joy of seeing your hero perform at the #FI...      0\n",
       "2   The joy of seeing your hero perform at the #FI...      0\n",
       "3   PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...      0\n",
       "4   PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...      0\n",
       "5   PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...      0\n",
       "6   PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...      0\n",
       "7   @miseleccionmxEN @miseleccionmx \\n$8.8 Million...      0\n",
       "8   Playing cricket in parkikg lot. LilLoquito fou...      0\n",
       "9   Okay last football tweet: Mexico‚Äôs football fe...      0\n",
       "10  NO PUNCH! üò∞\\n\\nZero goals for Mexico in Qatar ...      0\n",
       "11  Well Mexico never fails to disappoint me at th...      0\n",
       "12  The same energy \\n\\n#SeleccionMexicana #QatarW...      0\n",
       "13  Mexico couldn't convert when they were the bet...      0\n",
       "14  Mexico national team coach wanted his team to ...      0\n",
       "15  Mexico national team coach wanted his team to ...      0\n",
       "16  How the fuck do we go in to a WC with an Argen...      0\n",
       "17  PTM 2do Gol de #Argentina üá¶üá∑ alv 2Ô∏è‚É£‚ûñ0Ô∏è‚É£! Pobr...      0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mexico = pd.read_csv('data/all_Tweets_2022-11-28(Mexico).csv')\n",
    "df_mexico.head()\n",
    "#df_mexico.loc[df_mexico['lang']=='en']\n",
    "lnguage = 'en'\n",
    "# saving the data for mexico \n",
    "t_1=pd.DataFrame({'tweet':list(df_mexico.loc[df_mexico['lang']==lnguage]['text']),'senti':0})\n",
    "#t_1 = df_mexico['text']\n",
    "#t_1['class'] = 0\n",
    "t_1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3eb8cc-8964-444b-8a0a-3653406c49bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a286f16-5341-4660-b15f-9825583b0759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "129b1550-4dda-4c7e-9d46-adf48fb74300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1259)\n",
      "(2,)\n",
      "(2, 1260)\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3dc7e58dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/augusto/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ect = wall_e.predict_new(list(t_1['tweet'])[8])\n",
    "ect[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "171f2bb9-f6b6-4315-8632-535846cc032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cbf01-d9d3-44a7-8d43-da8477f00508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41083559-8a14-4385-9874-fbc94789cf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b60b2-b74a-449e-b054-587d1a2fd1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
