{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7312d2a-0732-438f-a3f5-9e4aeb353e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Robot for training \n",
    "\n",
    "## Introduction \n",
    "For this project, let us create the conpcept of a robot for NLP tasks \n",
    "here we are working with a class to controll the overall flow of the program. \n",
    "in order to have a more controlled environment to train. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f676fb06-9314-4b36-ac0b-46c7127edffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2000a75-c39e-4dea-a85b-da6895b638e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 21:13:56.247215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 21:13:56.662172: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-30 21:13:56.759717: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-30 21:13:56.759751: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-30 21:13:56.827750: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-30 21:13:57.971718: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 21:13:57.971882: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 21:13:57.971889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# set seeds for reproducability\n",
    "#from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2) \n",
    "from numpy.random import seed\n",
    "#set_random_seed(2)\n",
    "seed(1)\n",
    "# keras module for building LSTM \n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from keras.utils.data_utils import get_file\n",
    "import random\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77108a56-7447-48c6-b56a-659dc402db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf54562e-0d61-46c7-a59a-6b39653f5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class robot_predictor_trainable: \n",
    "    def __init__(self):\n",
    "        #here we have the global variables.so far we need \n",
    "        #1. the data frames \n",
    "        # the model architecture   \n",
    "        self.df_class_1 = None\n",
    "        self.df_class_2 = None\n",
    "        #the tokenizer for each class\n",
    "        self.tokenizer_class_1 = None\n",
    "        self.tokenizer_class_2 = None\n",
    "\n",
    "        #here we need one corpurs per each class\n",
    "        self.corpus_1 = None\n",
    "        self.corpus_2 = None\n",
    "        \n",
    "        #we need two different tokenizers\n",
    "        self.tokenizer_1 = Tokenizer()\n",
    "        self.tokenizer_2 = Tokenizer()\n",
    "        \n",
    "        #the models to save\n",
    "        self.model_1 = None\n",
    "        self.model_2 = None\n",
    "        \n",
    "        #Metadata for trainning class1\n",
    "        self.total_words_class1=None\n",
    "        self.predictors_class1=None\n",
    "        self.label_class1=None\n",
    "        self.max_sequence_len_class1=None\n",
    "        \n",
    "        #Metadata for trainning class2\n",
    "        self.total_words_class2=None\n",
    "        self.predictors_class2=None\n",
    "        self.label_class2=None\n",
    "        self.max_sequence_len_class2=None\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "    #methods for cleaning the tweets\n",
    "    #preprocessing methods\n",
    "    def scrub_words(self,text):\n",
    "        # remove html markup\n",
    "        import re\n",
    "        text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "        #remove non-ascii and digits\n",
    "        text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "  \n",
    "        #remove whitespace\n",
    "        text=text.strip()\n",
    "        return text\n",
    "        \n",
    "    #Noise removal, stop word removal, normalizing?\n",
    "    def cleanString(self,s, special_chars = \"\\\":,.@|√∞√ø≈ì≈æ√∞√ø√¢≈ì≈ì√Ø√ø≈ì≈æ√ø¬∫√ø√ø≈ì≈æ√ø\"):\n",
    "        from nltk.tokenize import TweetTokenizer\n",
    "        from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "        for char in special_chars:\n",
    "            s = s.replace(char, \"\")\n",
    "        s = s.replace(\"\\n\", \"\")\n",
    "        s = s.replace(\"https\", \"\")\n",
    "        s = self.scrub_words(s)\n",
    "        tokenizer = TweetTokenizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        cleaned_words = [w for w in tokenizer.tokenize(s) if w not in stop_words]\n",
    "        return \" \".join(cleaned_words)\n",
    "    \n",
    "    def cleanFrame(self,frame):\n",
    "        frame['clean_tweet'] = frame.tweet.apply(self.cleanString)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    #The methods avaialabe to Load the information  \n",
    "    def load_dataset_class_1(self):\n",
    "        df_mexico = pd.read_csv('mexico_en_.csv')\n",
    "        #lnguage = 'en'\n",
    "        #df_mexico_1 = df_mexico.loc[df_mexico['lang']==lnguage]        \n",
    "        t_1=pd.DataFrame({'tweet':list(df_mexico['text'])})\n",
    "        #load the data to the class\n",
    "        self.cleanFrame(t_1)\n",
    "        self.df_class_1 = t_1\n",
    "        t_1.to_csv('clean_data_c_1.csv')\n",
    "        print('Data loaded successfully')\n",
    "\n",
    "    def load_dataset_class_2(self):\n",
    "        df_argentina = pd.read_csv('argentina_en.csv')\n",
    "        #lnguage = 'en'\n",
    "        #df_argentina_1 = df_argentina.loc[df_argentina['lang']==lnguage]        \n",
    "        t_1=pd.DataFrame({'tweet':list(df_argentina['text'])})\n",
    "        #load the data to the class\n",
    "        self.cleanFrame(t_1)\n",
    "        self.df_class_2 = t_1\n",
    "        t_1.to_csv('clean_data_c_2.csv')\n",
    "\n",
    "        print('Data loaded successfully')\n",
    "        \n",
    "        \n",
    "    #=====================================================#\n",
    "    \n",
    "    \n",
    "    #once we have loaded the data we need to generate the corpus per\n",
    "    #each class, so far the methodology is the next, \n",
    "    \n",
    "    def clean_text(self,txt):\n",
    "        txt = \"\".join(t for t in txt if t not in string.punctuation).lower()\n",
    "        txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "        return txt\n",
    "    \n",
    "    \n",
    "    #tokenizer = Tokenizer()\n",
    "    def get_sequence_of_tokens_1(self,corpus):\n",
    "        ## tokenization\n",
    "        self.tokenizer_1.fit_on_texts(corpus)\n",
    "        total_words = len(self.tokenizer_1.word_index) + 1\n",
    "    \n",
    "        ## convert data to a token sequence \n",
    "        input_sequences = []\n",
    "        for line in corpus:\n",
    "            token_list = self.tokenizer_1.texts_to_sequences([line])[0]\n",
    "            for i in range(1, len(token_list)):\n",
    "                n_gram_sequence = token_list[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        return input_sequences, total_words\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_sequence_of_tokens_2(self,corpus):\n",
    "        ## tokenization\n",
    "        self.tokenizer_2.fit_on_texts(corpus)\n",
    "        total_words = len(self.tokenizer_2.word_index) + 1\n",
    "    \n",
    "        ## convert data to a token sequence \n",
    "        input_sequences = []\n",
    "        for line in corpus:\n",
    "            token_list = self.tokenizer_2.texts_to_sequences([line])[0]\n",
    "            for i in range(1, len(token_list)):\n",
    "                n_gram_sequence = token_list[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        return input_sequences, total_words\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_padded_sequences(self,input_sequences,total_words):\n",
    "        max_sequence_len = max([len(x) for x in input_sequences])\n",
    "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "        predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "        label = ku.to_categorical(label, num_classes=total_words)\n",
    "        return predictors, label, max_sequence_len\n",
    "    \n",
    "    #*******************\n",
    "    def create_model(self,max_sequence_len,total_words):\n",
    "        input_len = max_sequence_len - 1\n",
    "        model = Sequential()\n",
    "        # ----------Add Input Embedding Layer\n",
    "        model.add(Embedding(total_words,150, input_length=input_len))\n",
    "        # ----------Add Hidden Layer 1 - LSTM Layer\n",
    "        model.add(LSTM(700))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # ----------Add Output Layer\n",
    "        model.add(Dense(total_words, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        return model\n",
    "    \n",
    "    #>>>>>>>>>>>>>>>>>>>>>>>><\n",
    "    #this method is done to create the corpus and train a model in that corpus \n",
    "    \n",
    "    \n",
    "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    #self.total_words_class1=None\n",
    "    #self.predictors_class1=None\n",
    "    #self.label_class1=None\n",
    "    #self.max_sequence_len_class1=None\n",
    "    \n",
    "    def create_corpus_clas_1(self):\n",
    "        #create the corpus for self.df_class_1\n",
    "        all_headlines = list(self.df_class_1['clean_tweet'])\n",
    "        corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.corpus_1=corpus\n",
    "        print(corpus[:10])\n",
    "        #token created at the beginning\n",
    "        #tokenizer = Tokenizer()\n",
    "        #tokenize the corpus\n",
    "        inp_sequences,self.total_words_class1 = self.get_sequence_of_tokens_1(corpus)\n",
    "        print(inp_sequences[:10])\n",
    "        \n",
    "        #generate padding sequences \n",
    "        self.predictors_class1,self.label_class1,self.max_sequence_len_class1= self.generate_padded_sequences(inp_sequences,self.total_words_class1)\n",
    "        \n",
    "      \n",
    "        \n",
    "\n",
    "        \n",
    "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    \n",
    "    \n",
    "       \n",
    "    def create_corpus_clas_2(self):\n",
    "        #create the corpus for self.df_class_1\n",
    "        all_headlines = list(self.df_class_2['clean_tweet'])\n",
    "        corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.corpus_2=corpus\n",
    "        print(corpus[:10])\n",
    "        #token created at the beginning\n",
    "        #tokenizer = Tokenizer()\n",
    "        #tokenize the corpus\n",
    "        inp_sequences,self.total_words_class2 = self.get_sequence_of_tokens_2(corpus)\n",
    "        print(inp_sequences[:10])\n",
    "        \n",
    "        #generate padding sequences \n",
    "        self.predictors_class2,self.label_class2,self.max_sequence_len_class2= self.generate_padded_sequences(inp_sequences,self.total_words_class2)\n",
    "        \n",
    "      \n",
    "        \n",
    "    ##################################################\n",
    "    #Training \n",
    "    def train_class_1(self):\n",
    "        #use to train \n",
    "        model = self.create_model(self.max_sequence_len_class1,self.total_words_class1)\n",
    "        model.summary()\n",
    "        \n",
    "        model.fit(self.predictors_class1,self.label_class1, epochs=20, verbose=5)\n",
    "        self.model_1 = model\n",
    "        model.save('Training_class_1.h5')\n",
    "        \n",
    "        #Training \n",
    "    def train_class_2(self):\n",
    "        #use to train \n",
    "        model = self.create_model(self.max_sequence_len_class2,self.total_words_class2)\n",
    "        model.summary()\n",
    "        #print_callback = LambdaCallback(on_epoch_end=self.on_epoch_end)\n",
    "        model.fit(self.predictors_class2,self.label_class2, epochs=20, verbose=5)\n",
    "        self.model_2 = model\n",
    "        model.save('Training_class_2.h5')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    #call back functions\n",
    "    def sample(self,preds, temperature=1.0):\n",
    "        # helper function to sample an index from a probability array\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self,epoch, _):\n",
    "        # Function invoked at end of each epoch. Prints generated text.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print()\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "        text=str(self.corpus_2)\n",
    "        \n",
    "        chars = sorted(list(set(text)))\n",
    "        print('total chars:', len(chars))\n",
    "        char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "        indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "        # cut the text in semi-redundant sequences of maxlen characters\n",
    "        maxlen = 40\n",
    "        step = 3\n",
    "        sentences = []\n",
    "        next_chars = []\n",
    "        for i in range(0, len(text) - maxlen, step):\n",
    "            sentences.append(text[i: i + maxlen])\n",
    "            next_chars.append(text[i + maxlen])\n",
    "        print('nb sequences:', len(sentences))\n",
    "\n",
    "        #print('Vectorization...')\n",
    "        #x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "        #y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "        #for i, sentence in enumerate(sentences):\n",
    "         #   for t, char in enumerate(sentence):\n",
    "          #      x[i, t, char_indices[char]] = 1\n",
    "           # y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        maxlen=self.max_sequence_len_class2\n",
    "        start_index = random.randint(0,len(text) - maxlen - 1)\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + str(sentence) + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    #self.total_words_class1=None\n",
    "    #self.predictors_class1=None\n",
    "    #self.label_class1=None\n",
    "    #self.max_sequence_len_class1=None\n",
    "    #methods to generate Text \n",
    "    #here we need to create one tweet of a given class to the other class.\n",
    "    \n",
    "    # 1. FROM 1 to 2 \n",
    "    # 2. FROM 2 to 1\n",
    "    \n",
    "    \n",
    "    def generate_text_from_1_to_1(self,seed_text, next_words):\n",
    "        #the main workflow is twett_class_1 -> encode_class1-> z -> decode_class_2\n",
    "        model = tf.keras.models.load_model('Training_class_1.h5')\n",
    "        gen_w = \"\"\n",
    "        \n",
    "        #enconding_class1 \n",
    "        for _ in range(next_words):\n",
    "            token_list = self.tokenizer_1.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences([token_list],maxlen=self.max_sequence_len_class1-1, padding='pre')\n",
    "            #predicted = model.predict_classes(token_list, verbose=0)\n",
    "            #decode with model_class_2\n",
    "            predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "            output_word = \"\"\n",
    "            #descompress for argentina xd\n",
    "            for word,index in self.tokenizer_1.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            seed_text += \" \"+output_word\n",
    "            gen_w += \" \"+output_word\n",
    "            \n",
    "        return gen_w.title()\n",
    "    \n",
    "    \n",
    "\n",
    "    def generate_text_from_2_to_2(self,seed_text, next_words):\n",
    "        #the main workflow is twett_class_1 -> encode_class1-> z -> decode_class_2\n",
    "        model = tf.keras.models.load_model('Training_class_2.h5')\n",
    "        gen_w = \"\"\n",
    "        \n",
    "        #enconding_class1 \n",
    "        for _ in range(next_words):\n",
    "            token_list = self.tokenizer_2.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences([token_list],maxlen=self.max_sequence_len_class2-1, padding='pre')\n",
    "            #predicted = model.predict_classes(token_list, verbose=0)\n",
    "            #decode with model_class_2\n",
    "            predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "            output_word = \"\"\n",
    "            #descompress for argentina xd\n",
    "            for word,index in self.tokenizer_2.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            seed_text += \" \"+output_word\n",
    "            gen_w += \" \"+output_word\n",
    "            \n",
    "        return gen_w.title()\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef57749-34fd-4701-93c3-cb94d9dad9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e692ee53-8a5f-46fb-9a15-8227ec019b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the class\n",
    "robot = robot_predictor_trainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e47d5561-e662-4f7c-b34a-eeeb1ba05322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "#load the data \n",
    "\n",
    "robot.load_dataset_class_1()\n",
    "\n",
    "robot.load_dataset_class_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b63e50-6e9d-4216-b998-543df34880db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh wow ricardo la volpe mauro camoranesi starred fun moment discussed importance seleccionmexicana reaching fifth game qatar they also sent harsh message liga mx tco gmzkdhebby', 'many fans seleccinmexicana selling tickets mexico vs saudiarabia game allaboutqatar tco icrv e oty', 'meanwhile little princesses misnamed seleccionmexicana mind photograph relatives players score goalsbut hey expected fans still receive returning worldcupmiseleccionmx tco j gkjhz x', 'send us one seleccionmexicana one femexfutac teams give one forwards tco oc anzrb', 'it seen coming didn notice tata came vacation like eriksson difference eriksson sent hell months tata living almost years seleccionmexicana mexico qatar mundial', 'canelolvarez warns messi social media kicking mexicanselection shirt sports national international canelo qatar tco ytwdityh tco itbyauw', 'after threatening messi sergio kun agero cesc fabregas respond canelo lvarez tco nppixxwyej info info deportes canelo messi copamundialfifa mundial seleccionargentina seleccionmexicana tco vcrudptl h', 'tell miss chicharito legend lead mexico failure tata yondeluisa seleccionmexicana tco qdqmp sz', 'lamediocre seleccionmexicana brazil national team already round tco rhisrakl h', 'argentine actor responds canelo lvarez defends messi trampling mexican shirt qatar ag r']\n",
      "[[549, 940], [549, 940, 595], [549, 940, 595, 637], [549, 940, 595, 637, 1844], [549, 940, 595, 637, 1844, 4755], [549, 940, 595, 637, 1844, 4755, 3018], [549, 940, 595, 637, 1844, 4755, 3018, 4756], [549, 940, 595, 637, 1844, 4755, 3018, 4756, 741], [549, 940, 595, 637, 1844, 4755, 3018, 4756, 741, 395], [549, 940, 595, 637, 1844, 4755, 3018, 4756, 741, 395, 4757]]\n"
     ]
    }
   ],
   "source": [
    "#create copus class 1\n",
    "robot.create_corpus_clas_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc202421-f45c-4226-a10d-e36fa253a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all calves maria becerra releases album la nena de argentina tco vpwebz', 'lauradimarco taking period argentina conquered high salary us sthe default pays lie another q macriesmufa', 'bluish edutorresr antonionelli nono hippocrita would disgusted goals argentina celebrated hypothetical goals mexicoclosing comment volpe moderately known argentine coach really feels mexican flag argentina', 'edufek joke bad taste country kingdom upside argentine case beautiful country people work', 'eldi brunodigennaro alvaritomorales football game win argentina involve issues do need boys hug', 'kaeldelaj brunoformiga so switzerland better france saudi arabia team argentina copa america', 'the problem goes result belgium argentina copa america tco odhat zrg', 'ndyws yes yes take cat matches argentina mufeti family', 'the economist also defined plan argentina move forward negotiable implies reform state lowering public spending adjustment instead falling people falls political caste', 'bayernhost i realised wanted remove want argentina win loooool']\n",
      "[[190, 6115], [190, 6115, 565], [190, 6115, 565, 6116], [190, 6115, 565, 6116, 6117], [190, 6115, 565, 6116, 6117, 6118], [190, 6115, 565, 6116, 6117, 6118, 205], [190, 6115, 565, 6116, 6117, 6118, 205, 4031], [190, 6115, 565, 6116, 6117, 6118, 205, 4031, 86], [190, 6115, 565, 6116, 6117, 6118, 205, 4031, 86, 1], [190, 6115, 565, 6116, 6117, 6118, 205, 4031, 86, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "robot.create_corpus_clas_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe16af7-edfb-48a9-9b82-cb04240e4946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc72f20-aa9a-4bed-aef5-fa90fa377bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 21:14:10.218189: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-30 21:14:10.218452: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-30 21:14:10.218473: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (augusto-Aspire-A515-55): /proc/driver/nvidia/version does not exist\n",
      "2022-11-30 21:14:10.219367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 60, 150)           1669350   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 700)               2382800   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 700)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11129)             7801429   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,853,579\n",
      "Trainable params: 11,853,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "robot.train_class_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7cd268da-c5e3-4c19-b184-4cff28a647a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 26, 300)           80400     \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 1500)              10806000  \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 268)               402268    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,288,668\n",
      "Trainable params: 11,288,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "robot.train_class_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ac41c-6f00-4052-9247-1c39b7c025a7",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "644a6f21-76cb-4db4-af2c-64430a915870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>üìäNumbers of the #SeleccionMexicana üá≤üáΩ in World...</td>\n",
       "      <td>Numbers SeleccionMexicana World Cups Won Ties ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The joy of seeing your hero perform at the #FI...</td>\n",
       "      <td>The joy seeing hero perform FIFAWorldCup fifaw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The joy of seeing your hero perform at the #FI...</td>\n",
       "      <td>The joy seeing hero perform FIFAWorldCup fifaw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...</td>\n",
       "      <td>PAY US TO HELP IN YOUR DUEHomeworkAssignmentsE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...</td>\n",
       "      <td>PAY US TO HELP IN YOUR DUEHomeworkAssignmentsE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>143</td>\n",
       "      <td>üìå @equipoayb\\nüìÖ Saturday November 2nd 6th, 202...</td>\n",
       "      <td>equipoayb Saturday November nd th fifaworldcup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>üìå @equipoayb\\nüìÖ Saturday November 2nd 6th, 202...</td>\n",
       "      <td>equipoayb Saturday November nd th fifaworldcup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>145</td>\n",
       "      <td>üìå @equipoayb\\nüìÖ Saturday November 2nd 6th, 202...</td>\n",
       "      <td>equipoayb Saturday November nd th fifaworldcup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>@ALEXPAEZ29 @SirJohnLaguna @miseleccionmx True</td>\n",
       "      <td>ALEXPAEZ SirJohnLaguna miseleccionmx True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>@PrimeVideo @RafaMarquezMX @landondonovan @USM...</td>\n",
       "      <td>PrimeVideo RafaMarquezMX landondonovan USMNT m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                              tweet  \\\n",
       "0             0  üìäNumbers of the #SeleccionMexicana üá≤üáΩ in World...   \n",
       "1             1  The joy of seeing your hero perform at the #FI...   \n",
       "2             2  The joy of seeing your hero perform at the #FI...   \n",
       "3             3  PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...   \n",
       "4             4  PAY US TO HELP IN YOUR DUE\\n\\nHomework\\nAssign...   \n",
       "..          ...                                                ...   \n",
       "143         143  üìå @equipoayb\\nüìÖ Saturday November 2nd 6th, 202...   \n",
       "144         144  üìå @equipoayb\\nüìÖ Saturday November 2nd 6th, 202...   \n",
       "145         145  üìå @equipoayb\\nüìÖ Saturday November 2nd 6th, 202...   \n",
       "146         146     @ALEXPAEZ29 @SirJohnLaguna @miseleccionmx True   \n",
       "147         147  @PrimeVideo @RafaMarquezMX @landondonovan @USM...   \n",
       "\n",
       "                                           clean_tweet  \n",
       "0    Numbers SeleccionMexicana World Cups Won Ties ...  \n",
       "1    The joy seeing hero perform FIFAWorldCup fifaw...  \n",
       "2    The joy seeing hero perform FIFAWorldCup fifaw...  \n",
       "3    PAY US TO HELP IN YOUR DUEHomeworkAssignmentsE...  \n",
       "4    PAY US TO HELP IN YOUR DUEHomeworkAssignmentsE...  \n",
       "..                                                 ...  \n",
       "143  equipoayb Saturday November nd th fifaworldcup...  \n",
       "144  equipoayb Saturday November nd th fifaworldcup...  \n",
       "145  equipoayb Saturday November nd th fifaworldcup...  \n",
       "146          ALEXPAEZ SirJohnLaguna miseleccionmx True  \n",
       "147  PrimeVideo RafaMarquezMX landondonovan USMNT m...  \n",
       "\n",
       "[148 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('clean_data_c_1.csv')\n",
    "#df.head()\n",
    "c_1 = list(df['clean_tweet'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b8eb2aa-d2e8-4094-9d8d-af0c939f553d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@LigaAFA_Eng @bff_football @JohnCena Unbelieva...</td>\n",
       "      <td>LigaAFA_Eng bff_football JohnCena Unbelievable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@budweiser_ar Unbelievable crowd to watch Arge...</td>\n",
       "      <td>budweiser_ar Unbelievable crowd watch Argentin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@golazoargentino Unbelievable crowd to watch A...</td>\n",
       "      <td>golazoargentino Unbelievable crowd watch Argen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@sudanalytics_ Unbelievable crowd to watch Arg...</td>\n",
       "      <td>sudanalytics _ Unbelievable crowd watch Argent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>We are not going to give up.ü§ô\\nWe are Argentin...</td>\n",
       "      <td>We going give We Argentina Selecci√≥nmayor Albi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>@dataref_ar Unbelievable crowd to watch Argent...</td>\n",
       "      <td>dataref_ar Unbelievable crowd watch Argentina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>@DiarioOle @SobugMessi10jr Unbelievable crowd ...</td>\n",
       "      <td>DiarioOle SobugMessi jr Unbelievable crowd wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>@ESPNFC @SobugMessi10jr Unbelievable crowd to ...</td>\n",
       "      <td>ESPNFC SobugMessi jr Unbelievable crowd watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>@sudanalytics_ Unbelievable crowd to watch Arg...</td>\n",
       "      <td>sudanalytics _ Unbelievable crowd watch Argent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>We are not going to give up.ü§ô\\nWe are Argentin...</td>\n",
       "      <td>We going give We Argentina Selecci√≥nmayor Albi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>@dataref_ar Unbelievable crowd to watch Argent...</td>\n",
       "      <td>dataref_ar Unbelievable crowd watch Argentina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>@DiarioOle @SobugMessi10jr Unbelievable crowd ...</td>\n",
       "      <td>DiarioOle SobugMessi jr Unbelievable crowd wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>@ESPNFC @SobugMessi10jr Unbelievable crowd to ...</td>\n",
       "      <td>ESPNFC SobugMessi jr Unbelievable crowd watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>@sudanalytics_ Unbelievable crowd to watch Arg...</td>\n",
       "      <td>sudanalytics _ Unbelievable crowd watch Argent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>We are not going to give up.ü§ô\\nWe are Argentin...</td>\n",
       "      <td>We going give We Argentina Selecci√≥nmayor Albi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>@dataref_ar Unbelievable crowd to watch Argent...</td>\n",
       "      <td>dataref_ar Unbelievable crowd watch Argentina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>@DiarioOle @SobugMessi10jr Unbelievable crowd ...</td>\n",
       "      <td>DiarioOle SobugMessi jr Unbelievable crowd wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>@ESPNFC @SobugMessi10jr Unbelievable crowd to ...</td>\n",
       "      <td>ESPNFC SobugMessi jr Unbelievable crowd watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Unbelievable crowd to watch Argentina-Mexico m...</td>\n",
       "      <td>Unbelievable crowd watch Argentina Mexico matc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>@dataref_ar from Bangladesh üá¶üá∑üíôüáßüá©\\n\\n#majorsel...</td>\n",
       "      <td>dataref_ar Bangladesh majorselection albiceles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\nA brilliant finish...</td>\n",
       "      <td>WorldCup Qatar A brilliant finish assist Lione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 87' | What a fin...</td>\n",
       "      <td>WorldCup Qatar What finish Fernandez put Argen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 73' | Lozano mak...</td>\n",
       "      <td>WorldCup Qatar Lozano makes way Alvarado ARG M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 69' | Di Maria a...</td>\n",
       "      <td>WorldCup Qatar Di Maria ans Mac Allister repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 67' | Herrera bo...</td>\n",
       "      <td>WorldCup Qatar Herrera booked ARG MEX Selecci√≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞63' | Double chan...</td>\n",
       "      <td>WorldCup Qatar Double change ArgentinaMontiel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 57' | Rodriguez ...</td>\n",
       "      <td>WorldCup Qatar Rodriguez makes way Fernandez A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 50' | Gutierrez ...</td>\n",
       "      <td>WorldCup Qatar Gutierrez booked Free kick Arge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 46' | Back for t...</td>\n",
       "      <td>WorldCup Qatar Back second half ARG MEX Selecc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n#ARG and #MEX were...</td>\n",
       "      <td>WorldCup Qatar ARG MEX goalless end first half...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 44' | What a sav...</td>\n",
       "      <td>WorldCup Qatar What save Martinez ARG MEX Sele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 43' | Montiel bo...</td>\n",
       "      <td>WorldCup Qatar Montiel booked foul Gutierrez A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 41' | Saved make...</td>\n",
       "      <td>WorldCup Qatar Saved makes way Gutierrez ARG M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>#Argentina vs #Mexico Full Match &amp; #Highlights...</td>\n",
       "      <td>Argentina vs Mexico Full Match Highlights Repl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 17' | Argentina ...</td>\n",
       "      <td>WorldCup Qatar Argentina currently hold ball p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 14' | Handball, ...</td>\n",
       "      <td>WorldCup Qatar Handball free kick Argentina AR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 09' | Hirving Lo...</td>\n",
       "      <td>WorldCup Qatar Hirving Lozano wins free kick A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 08' | Mexico ear...</td>\n",
       "      <td>WorldCup Qatar Mexico earn first corner game A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 06' | Montiel is...</td>\n",
       "      <td>WorldCup Qatar Montiel currently receiving tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 05' | Vega foule...</td>\n",
       "      <td>WorldCup Qatar Vega fouled Montiel ARGMEX Sele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 05' | Free kick ...</td>\n",
       "      <td>WorldCup Qatar Free kick Argentina ARGMEX Sele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 01' | Mexico hav...</td>\n",
       "      <td>WorldCup Qatar Mexico made kick ARGMEX Selecci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>üèÜ #WorldCup | #Qatar2022\\n\\nREADY TO START: Th...</td>\n",
       "      <td>WorldCup Qatar READY TO START The teams line k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>#ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...</td>\n",
       "      <td>ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>Argentina take on Group C rivals Mexico eyeing...</td>\n",
       "      <td>Argentina take Group C rivals Mexico eyeing no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>#ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...</td>\n",
       "      <td>ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>#ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...</td>\n",
       "      <td>ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>Argentina take on Group C rivals Mexico eyeing...</td>\n",
       "      <td>Argentina take Group C rivals Mexico eyeing no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>#ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...</td>\n",
       "      <td>ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>#ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...</td>\n",
       "      <td>ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>Argentina take on Group C rivals Mexico eyeing...</td>\n",
       "      <td>Argentina take Group C rivals Mexico eyeing no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>#ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...</td>\n",
       "      <td>ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>@Argentina supporters üá¶üá∑\\n@Argentina\\n üèÜ #Sele...</td>\n",
       "      <td>Argentina supporters Argentina Selecci√≥nMayor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>both the ball and the second-last opponent. Th...</td>\n",
       "      <td>ball second last opponent The arms included de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>#Argentina vs #SaudiArabia #ArabiaSaudita Full...</td>\n",
       "      <td>Argentina vs SaudiArabia ArabiaSaudita Full Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>My review of #WorldCup2022 Group C before üá¶üá∑üÜöüá∏...</td>\n",
       "      <td>My review WorldCup Group C tco gb FgV gQt FIFA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>#ARGKSA\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...</td>\n",
       "      <td>ARGKSA Qatar QatarWorldCup FIFAWorldCup Selecc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>Argentina vs Saudi Arabia\\n\\nPixelChallenge! C...</td>\n",
       "      <td>Argentina vs Saudi ArabiaPixelChallenge Can pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>#ARGKSA\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...</td>\n",
       "      <td>ARGKSA Qatar QatarWorldCup FIFAWorldCup Selecc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                              tweet  \\\n",
       "0            0  @LigaAFA_Eng @bff_football @JohnCena Unbelieva...   \n",
       "1            1  @budweiser_ar Unbelievable crowd to watch Arge...   \n",
       "2            2  @golazoargentino Unbelievable crowd to watch A...   \n",
       "3            3  @sudanalytics_ Unbelievable crowd to watch Arg...   \n",
       "4            4  We are not going to give up.ü§ô\\nWe are Argentin...   \n",
       "5            5  @dataref_ar Unbelievable crowd to watch Argent...   \n",
       "6            6  @DiarioOle @SobugMessi10jr Unbelievable crowd ...   \n",
       "7            7  @ESPNFC @SobugMessi10jr Unbelievable crowd to ...   \n",
       "8            8  @sudanalytics_ Unbelievable crowd to watch Arg...   \n",
       "9            9  We are not going to give up.ü§ô\\nWe are Argentin...   \n",
       "10          10  @dataref_ar Unbelievable crowd to watch Argent...   \n",
       "11          11  @DiarioOle @SobugMessi10jr Unbelievable crowd ...   \n",
       "12          12  @ESPNFC @SobugMessi10jr Unbelievable crowd to ...   \n",
       "13          13  @sudanalytics_ Unbelievable crowd to watch Arg...   \n",
       "14          14  We are not going to give up.ü§ô\\nWe are Argentin...   \n",
       "15          15  @dataref_ar Unbelievable crowd to watch Argent...   \n",
       "16          16  @DiarioOle @SobugMessi10jr Unbelievable crowd ...   \n",
       "17          17  @ESPNFC @SobugMessi10jr Unbelievable crowd to ...   \n",
       "18          18  Unbelievable crowd to watch Argentina-Mexico m...   \n",
       "19          19  @dataref_ar from Bangladesh üá¶üá∑üíôüáßüá©\\n\\n#majorsel...   \n",
       "20          20  üèÜ #WorldCup | #Qatar2022\\n\\nA brilliant finish...   \n",
       "21          21  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 87' | What a fin...   \n",
       "22          22  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 73' | Lozano mak...   \n",
       "23          23  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 69' | Di Maria a...   \n",
       "24          24  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 67' | Herrera bo...   \n",
       "25          25  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞63' | Double chan...   \n",
       "26          26  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 57' | Rodriguez ...   \n",
       "27          27  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 50' | Gutierrez ...   \n",
       "28          28  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 46' | Back for t...   \n",
       "29          29  üèÜ #WorldCup | #Qatar2022\\n\\n#ARG and #MEX were...   \n",
       "30          30  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 44' | What a sav...   \n",
       "31          31  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 43' | Montiel bo...   \n",
       "32          32  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 41' | Saved make...   \n",
       "33          33  #Argentina vs #Mexico Full Match & #Highlights...   \n",
       "34          34  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 17' | Argentina ...   \n",
       "35          35  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 14' | Handball, ...   \n",
       "36          36  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 09' | Hirving Lo...   \n",
       "37          37  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 08' | Mexico ear...   \n",
       "38          38  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 06' | Montiel is...   \n",
       "39          39  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 05' | Vega foule...   \n",
       "40          40  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 05' | Free kick ...   \n",
       "41          41  üèÜ #WorldCup | #Qatar2022\\n\\n‚è∞ 01' | Mexico hav...   \n",
       "42          42  üèÜ #WorldCup | #Qatar2022\\n\\nREADY TO START: Th...   \n",
       "43          43  #ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...   \n",
       "44          44  Argentina take on Group C rivals Mexico eyeing...   \n",
       "45          45  #ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...   \n",
       "46          46  #ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...   \n",
       "47          47  Argentina take on Group C rivals Mexico eyeing...   \n",
       "48          48  #ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...   \n",
       "49          49  #ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...   \n",
       "50          50  Argentina take on Group C rivals Mexico eyeing...   \n",
       "51          51  #ARGMEX\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...   \n",
       "52          52  @Argentina supporters üá¶üá∑\\n@Argentina\\n üèÜ #Sele...   \n",
       "53          53  both the ball and the second-last opponent. Th...   \n",
       "54          54  #Argentina vs #SaudiArabia #ArabiaSaudita Full...   \n",
       "55          55  My review of #WorldCup2022 Group C before üá¶üá∑üÜöüá∏...   \n",
       "56          56  #ARGKSA\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...   \n",
       "57          57  Argentina vs Saudi Arabia\\n\\nPixelChallenge! C...   \n",
       "58          58  #ARGKSA\\n\\n#Qatar2022\\n\\n#QatarWorldCup,\\n\\n#F...   \n",
       "\n",
       "                                          clean_tweet  \n",
       "0   LigaAFA_Eng bff_football JohnCena Unbelievable...  \n",
       "1   budweiser_ar Unbelievable crowd watch Argentin...  \n",
       "2   golazoargentino Unbelievable crowd watch Argen...  \n",
       "3   sudanalytics _ Unbelievable crowd watch Argent...  \n",
       "4   We going give We Argentina Selecci√≥nmayor Albi...  \n",
       "5   dataref_ar Unbelievable crowd watch Argentina ...  \n",
       "6   DiarioOle SobugMessi jr Unbelievable crowd wat...  \n",
       "7   ESPNFC SobugMessi jr Unbelievable crowd watch ...  \n",
       "8   sudanalytics _ Unbelievable crowd watch Argent...  \n",
       "9   We going give We Argentina Selecci√≥nmayor Albi...  \n",
       "10  dataref_ar Unbelievable crowd watch Argentina ...  \n",
       "11  DiarioOle SobugMessi jr Unbelievable crowd wat...  \n",
       "12  ESPNFC SobugMessi jr Unbelievable crowd watch ...  \n",
       "13  sudanalytics _ Unbelievable crowd watch Argent...  \n",
       "14  We going give We Argentina Selecci√≥nmayor Albi...  \n",
       "15  dataref_ar Unbelievable crowd watch Argentina ...  \n",
       "16  DiarioOle SobugMessi jr Unbelievable crowd wat...  \n",
       "17  ESPNFC SobugMessi jr Unbelievable crowd watch ...  \n",
       "18  Unbelievable crowd watch Argentina Mexico matc...  \n",
       "19  dataref_ar Bangladesh majorselection albiceles...  \n",
       "20  WorldCup Qatar A brilliant finish assist Lione...  \n",
       "21  WorldCup Qatar What finish Fernandez put Argen...  \n",
       "22  WorldCup Qatar Lozano makes way Alvarado ARG M...  \n",
       "23  WorldCup Qatar Di Maria ans Mac Allister repla...  \n",
       "24  WorldCup Qatar Herrera booked ARG MEX Selecci√≥...  \n",
       "25  WorldCup Qatar Double change ArgentinaMontiel ...  \n",
       "26  WorldCup Qatar Rodriguez makes way Fernandez A...  \n",
       "27  WorldCup Qatar Gutierrez booked Free kick Arge...  \n",
       "28  WorldCup Qatar Back second half ARG MEX Selecc...  \n",
       "29  WorldCup Qatar ARG MEX goalless end first half...  \n",
       "30  WorldCup Qatar What save Martinez ARG MEX Sele...  \n",
       "31  WorldCup Qatar Montiel booked foul Gutierrez A...  \n",
       "32  WorldCup Qatar Saved makes way Gutierrez ARG M...  \n",
       "33  Argentina vs Mexico Full Match Highlights Repl...  \n",
       "34  WorldCup Qatar Argentina currently hold ball p...  \n",
       "35  WorldCup Qatar Handball free kick Argentina AR...  \n",
       "36  WorldCup Qatar Hirving Lozano wins free kick A...  \n",
       "37  WorldCup Qatar Mexico earn first corner game A...  \n",
       "38  WorldCup Qatar Montiel currently receiving tre...  \n",
       "39  WorldCup Qatar Vega fouled Montiel ARGMEX Sele...  \n",
       "40  WorldCup Qatar Free kick Argentina ARGMEX Sele...  \n",
       "41  WorldCup Qatar Mexico made kick ARGMEX Selecci...  \n",
       "42  WorldCup Qatar READY TO START The teams line k...  \n",
       "43  ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...  \n",
       "44  Argentina take Group C rivals Mexico eyeing no...  \n",
       "45  ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...  \n",
       "46  ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...  \n",
       "47  Argentina take Group C rivals Mexico eyeing no...  \n",
       "48  ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...  \n",
       "49  ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...  \n",
       "50  Argentina take Group C rivals Mexico eyeing no...  \n",
       "51  ARGMEX Qatar QatarWorldCup FIFAWorldCup Selecc...  \n",
       "52  Argentina supporters Argentina Selecci√≥nMayor ...  \n",
       "53  ball second last opponent The arms included de...  \n",
       "54  Argentina vs SaudiArabia ArabiaSaudita Full Ma...  \n",
       "55  My review WorldCup Group C tco gb FgV gQt FIFA...  \n",
       "56  ARGKSA Qatar QatarWorldCup FIFAWorldCup Selecc...  \n",
       "57  Argentina vs Saudi ArabiaPixelChallenge Can pr...  \n",
       "58  ARGKSA Qatar QatarWorldCup FIFAWorldCup Selecc...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv('clean_data_c_2.csv')\n",
    "#df.head()\n",
    "c_2 = list(df2['clean_tweet'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "08083447-ee54-42b3-910b-47348b97634c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One last dance SiSePuede tco GgVPMf zL'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_1[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "127830a9-47b0-4ea4-b04e-9b6e74179182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sobugmessi Jr Unbelievable Crowd Watch Argentina Mexico Match Dhaka Universitybangladesh Seleccinmayor Albicelestaforever Vamosargentina Mexicovsargentina Messi Worldcup Argentina Goat Bangladesh Worldcupqatar Qatar Tco Eim I K Eu Eu Eu Eu Eu Eu Zp Zp Zp Zp Zp Zp Zp Zp Zp Wx Wx Definition Nowisall Nowisall Nowisall Nowisall Tco Wx Wx Nowisall Worldcupqatar Worldcupqatar Worldcupqatar Tco Tj Cqkjkbx Fky Ee Mundialqatar'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot.generate_text_from_2_to_2(seed_text=c_1[56],next_words=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e96e0c7c-8e1d-4e5f-8f20-b90df210b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 485ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Team Team Team Win Miseleccionmxen Fifaworldcup Fifaworldcup Seleccionmexicana Seleccionmexicana Seleccionmexicana Argentina Mqj Mqj Polska Polska Saudiarabia Poland Tco Qwvypilkut Tco Qwvypilkut Tco Bzzeyrk N N N N N Ivq Ivq Ivq Ivq H H H H H H H Relying Football Veterans Veterans Need We Keep Athletes Athletes Mexico Mexico Football Qatar Seleccionmexicana Qatar Qatar Worldcup Mexicodemivida Mexicodemivida Tco Fspav Poland Tco Qwvypilkut Qwvypilkut Qwvypilkut Tco Qwvypilkut Tco N H H H H Ivq Ivq Ivq H H H H H H H H Tos Difference Seleccionmexicana Qatar Qatar Qatar Worldcup Afa Mexicodemivida Polska Ch Poland Tco Qwvypilkut Ivq Ivq Ivq Tco Ivq Ivq Ivq Ivq Ivq Ivq Ivq H H H H H H H H H H Npz Npz H Difference Difference Difference Argentina Athletes Mexico Seleccionmexicana Qatar Qatar Tco Fspav Tco Fspav Ivq Ivq Ivq Ivq Ivq'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot.generate_text_from_1_to_1(seed_text=c_2[0],next_words=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac49279-ecca-4f6c-b457-03c45530f2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9213e-c9fb-4b26-8a59-bf2d0dda2bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3ac43-453b-42b8-921d-5757dd4ad25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdedd7-e52b-4753-9647-54a88f5fd7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bf9e1-dcdc-44da-b51f-8af161d22a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd3245-6ab1-4af2-aa27-1a4cc881b75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70b682-4f1b-40b7-bdcb-29b6abe10653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aafae7-099f-4599-a25f-aab48a664a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
