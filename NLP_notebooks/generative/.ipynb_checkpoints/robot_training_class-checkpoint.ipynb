{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7312d2a-0732-438f-a3f5-9e4aeb353e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Robot for training \n",
    "\n",
    "## Introduction \n",
    "For this project, let us create the conpcept of a robot for NLP tasks \n",
    "here we are working with a class to controll the overall flow of the program. \n",
    "in order to have a more controlled environment to train. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f676fb06-9314-4b36-ac0b-46c7127edffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2000a75-c39e-4dea-a85b-da6895b638e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducability\n",
    "#from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2) \n",
    "from numpy.random import seed\n",
    "#set_random_seed(2)\n",
    "seed(1)\n",
    "# keras module for building LSTM \n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from keras.utils.data_utils import get_file\n",
    "import random\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77108a56-7447-48c6-b56a-659dc402db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf54562e-0d61-46c7-a59a-6b39653f5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class robot_predictor_trainable: \n",
    "    def __init__(self):\n",
    "        #here we have the global variables.so far we need \n",
    "        #1. the data frames \n",
    "        # the model architecture   \n",
    "        self.df_class_1 = None\n",
    "        self.df_class_2 = None\n",
    "        #the tokenizer for each class\n",
    "        self.tokenizer_class_1 = None\n",
    "        self.tokenizer_class_2 = None\n",
    "\n",
    "        #here we need one corpurs per each class\n",
    "        self.corpus_1 = None\n",
    "        self.corpus_2 = None\n",
    "        \n",
    "        #we need two different tokenizers\n",
    "        self.tokenizer_1 = Tokenizer()\n",
    "        self.tokenizer_2 = Tokenizer()\n",
    "        \n",
    "        #the models to save\n",
    "        self.model_1 = None\n",
    "        self.model_2 = None\n",
    "        \n",
    "        #Metadata for trainning class1\n",
    "        self.total_words_class1=None\n",
    "        self.predictors_class1=None\n",
    "        self.label_class1=None\n",
    "        self.max_sequence_len_class1=None\n",
    "        \n",
    "        #Metadata for trainning class2\n",
    "        self.total_words_class2=None\n",
    "        self.predictors_class2=None\n",
    "        self.label_class2=None\n",
    "        self.max_sequence_len_class2=None\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "    #methods for cleaning the tweets\n",
    "    #preprocessing methods\n",
    "    def scrub_words(self,text):\n",
    "        # remove html markup\n",
    "        import re\n",
    "        text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "        #remove non-ascii and digits\n",
    "        text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "  \n",
    "        #remove whitespace\n",
    "        text=text.strip()\n",
    "        return text\n",
    "        \n",
    "    #Noise removal, stop word removal, normalizing?\n",
    "    def cleanString(self,s, special_chars = \"\\\":,.@|√∞√ø≈ì≈æ√∞√ø√¢≈ì≈ì√Ø√ø≈ì≈æ√ø¬∫√ø√ø≈ì≈æ√ø\"):\n",
    "        from nltk.tokenize import TweetTokenizer\n",
    "        from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "        for char in special_chars:\n",
    "            s = s.replace(char, \"\")\n",
    "        s = s.replace(\"\\n\", \"\")\n",
    "        s = s.replace(\"https\", \"\")\n",
    "        s = self.scrub_words(s)\n",
    "        tokenizer = TweetTokenizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        cleaned_words = [w for w in tokenizer.tokenize(s) if w not in stop_words]\n",
    "        return \" \".join(cleaned_words)\n",
    "    \n",
    "    def cleanFrame(self,frame):\n",
    "        frame['clean_tweet'] = frame.tweet.apply(self.cleanString)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    #The methods avaialabe to Load the information  \n",
    "    def load_dataset_class_1(self):\n",
    "        df_mexico = pd.read_csv('allM.csv')\n",
    "        #lnguage = 'en'\n",
    "        #df_mexico_1 = df_mexico.loc[df_mexico['lang']==lnguage]        \n",
    "        t_1=pd.DataFrame({'tweet':list(df_mexico['text'])})\n",
    "        #load the data to the class\n",
    "        self.cleanFrame(t_1)\n",
    "        self.df_class_1 = t_1\n",
    "        t_1.to_csv('clean_data_c_1.csv')\n",
    "        print('Data loaded successfully')\n",
    "\n",
    "    def load_dataset_class_2(self):\n",
    "        df_argentina = pd.read_csv('allA.csv')\n",
    "        #lnguage = 'en'\n",
    "        #df_argentina_1 = df_argentina.loc[df_argentina['lang']==lnguage]        \n",
    "        t_1=pd.DataFrame({'tweet':list(df_argentina['text'])})\n",
    "        #load the data to the class\n",
    "        self.cleanFrame(t_1)\n",
    "        self.df_class_2 = t_1\n",
    "        t_1.to_csv('clean_data_c_2.csv')\n",
    "\n",
    "        print('Data loaded successfully')\n",
    "        \n",
    "        \n",
    "    #=====================================================#\n",
    "    \n",
    "    \n",
    "    #once we have loaded the data we need to generate the corpus per\n",
    "    #each class, so far the methodology is the next, \n",
    "    \n",
    "    def clean_text(self,txt):\n",
    "        txt = \"\".join(t for t in txt if t not in string.punctuation).lower()\n",
    "        txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "        return txt\n",
    "    \n",
    "    \n",
    "    #tokenizer = Tokenizer()\n",
    "    def get_sequence_of_tokens_1(self,corpus):\n",
    "        ## tokenization\n",
    "        self.tokenizer_1.fit_on_texts(corpus)\n",
    "        total_words = len(self.tokenizer_1.word_index) + 1\n",
    "    \n",
    "        ## convert data to a token sequence \n",
    "        input_sequences = []\n",
    "        for line in corpus:\n",
    "            token_list = self.tokenizer_1.texts_to_sequences([line])[0]\n",
    "            for i in range(1, len(token_list)):\n",
    "                n_gram_sequence = token_list[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        return input_sequences, total_words\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_sequence_of_tokens_2(self,corpus):\n",
    "        ## tokenization\n",
    "        self.tokenizer_2.fit_on_texts(corpus)\n",
    "        total_words = len(self.tokenizer_2.word_index) + 1\n",
    "    \n",
    "        ## convert data to a token sequence \n",
    "        input_sequences = []\n",
    "        for line in corpus:\n",
    "            token_list = self.tokenizer_2.texts_to_sequences([line])[0]\n",
    "            for i in range(1, len(token_list)):\n",
    "                n_gram_sequence = token_list[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        return input_sequences, total_words\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_padded_sequences(self,input_sequences,total_words):\n",
    "        max_sequence_len = max([len(x) for x in input_sequences])\n",
    "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "        predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "        label = ku.to_categorical(label, num_classes=total_words)\n",
    "        return predictors, label, max_sequence_len\n",
    "    \n",
    "    #*******************\n",
    "    def create_model(self,max_sequence_len,total_words):\n",
    "        input_len = max_sequence_len - 1\n",
    "        model = Sequential()\n",
    "        # ----------Add Input Embedding Layer\n",
    "        model.add(Embedding(total_words,80, input_length=input_len))\n",
    "        # ----------Add Hidden Layer 1 - LSTM Layer\n",
    "        model.add(LSTM(700))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        # ----------Add Output Layer\n",
    "        model.add(Dense(total_words, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        return model\n",
    "    \n",
    "    #>>>>>>>>>>>>>>>>>>>>>>>><\n",
    "    #this method is done to create the corpus and train a model in that corpus \n",
    "    \n",
    "    \n",
    "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    #self.total_words_class1=None\n",
    "    #self.predictors_class1=None\n",
    "    #self.label_class1=None\n",
    "    #self.max_sequence_len_class1=None\n",
    "    \n",
    "    def create_corpus_clas_1(self):\n",
    "        #create the corpus for self.df_class_1\n",
    "        all_headlines = list(self.df_class_1['clean_tweet'][0:2000])\n",
    "        corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.corpus_1=corpus\n",
    "        print(corpus[:10])\n",
    "        #token created at the beginning\n",
    "        #tokenizer = Tokenizer()\n",
    "        #tokenize the corpus\n",
    "        inp_sequences,self.total_words_class1 = self.get_sequence_of_tokens_1(corpus)\n",
    "        print(inp_sequences[:10])\n",
    "        \n",
    "        #generate padding sequences \n",
    "        self.predictors_class1,self.label_class1,self.max_sequence_len_class1= self.generate_padded_sequences(inp_sequences,self.total_words_class1)\n",
    "        \n",
    "      \n",
    "        \n",
    "\n",
    "        \n",
    "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    \n",
    "    \n",
    "       \n",
    "    def create_corpus_clas_2(self):\n",
    "        #create the corpus for self.df_class_1\n",
    "        all_headlines = list(self.df_class_2['clean_tweet'][0:2000])\n",
    "        corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.corpus_2=corpus\n",
    "        print(corpus[:10])\n",
    "        #token created at the beginning\n",
    "        #tokenizer = Tokenizer()\n",
    "        #tokenize the corpus\n",
    "        inp_sequences,self.total_words_class2 = self.get_sequence_of_tokens_2(corpus)\n",
    "        print(inp_sequences[:10])\n",
    "        \n",
    "        #generate padding sequences \n",
    "        self.predictors_class2,self.label_class2,self.max_sequence_len_class2= self.generate_padded_sequences(inp_sequences,self.total_words_class2)\n",
    "        \n",
    "      \n",
    "        \n",
    "    ##################################################\n",
    "    #Training \n",
    "    def train_class_1(self):\n",
    "        #use to train \n",
    "        model = self.create_model(self.max_sequence_len_class1,self.total_words_class1)\n",
    "        model.summary()\n",
    "        \n",
    "        model.fit(self.predictors_class1,self.label_class1, epochs=20, verbose=5)\n",
    "        self.model_1 = model\n",
    "        model.save('Training_class_1.h5')\n",
    "        \n",
    "        #Training \n",
    "    def train_class_2(self):\n",
    "        #use to train \n",
    "        model = self.create_model(self.max_sequence_len_class2,self.total_words_class2)\n",
    "        model.summary()\n",
    "        #print_callback = LambdaCallback(on_epoch_end=self.on_epoch_end)\n",
    "        model.fit(self.predictors_class2,self.label_class2, epochs=20, verbose=5)\n",
    "        self.model_2 = model\n",
    "        model.save('Training_class_2.h5')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    #call back functions\n",
    "    def sample(self,preds, temperature=1.0):\n",
    "        # helper function to sample an index from a probability array\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self,epoch, _):\n",
    "        # Function invoked at end of each epoch. Prints generated text.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print()\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "        text=str(self.corpus_2)\n",
    "        \n",
    "        chars = sorted(list(set(text)))\n",
    "        print('total chars:', len(chars))\n",
    "        char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "        indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "        # cut the text in semi-redundant sequences of maxlen characters\n",
    "        maxlen = 40\n",
    "        step = 3\n",
    "        sentences = []\n",
    "        next_chars = []\n",
    "        for i in range(0, len(text) - maxlen, step):\n",
    "            sentences.append(text[i: i + maxlen])\n",
    "            next_chars.append(text[i + maxlen])\n",
    "        print('nb sequences:', len(sentences))\n",
    "\n",
    "        #print('Vectorization...')\n",
    "        #x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "        #y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "        #for i, sentence in enumerate(sentences):\n",
    "         #   for t, char in enumerate(sentence):\n",
    "          #      x[i, t, char_indices[char]] = 1\n",
    "           # y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        maxlen=self.max_sequence_len_class2\n",
    "        start_index = random.randint(0,len(text) - maxlen - 1)\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + str(sentence) + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    #self.total_words_class1=None\n",
    "    #self.predictors_class1=None\n",
    "    #self.label_class1=None\n",
    "    #self.max_sequence_len_class1=None\n",
    "    #methods to generate Text \n",
    "    #here we need to create one tweet of a given class to the other class.\n",
    "    \n",
    "    # 1. FROM 1 to 2 \n",
    "    # 2. FROM 2 to 1\n",
    "    \n",
    "    \n",
    "    def generate_text_from_1_to_1(self,seed_text, next_words):\n",
    "        #the main workflow is twett_class_1 -> encode_class1-> z -> decode_class_2\n",
    "        model = tf.keras.models.load_model('Training_class_1.h5')\n",
    "        gen_w = \"\"\n",
    "        \n",
    "        #enconding_class1 \n",
    "        for _ in range(next_words):\n",
    "            token_list = self.tokenizer_1.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences([token_list],maxlen=self.max_sequence_len_class1-1, padding='pre')\n",
    "            #predicted = model.predict_classes(token_list, verbose=0)\n",
    "            #decode with model_class_2\n",
    "            predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "            output_word = \"\"\n",
    "            #descompress for argentina xd\n",
    "            for word,index in self.tokenizer_1.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            seed_text += \" \"+output_word\n",
    "            gen_w += \" \"+output_word\n",
    "            \n",
    "        return gen_w.title()\n",
    "    \n",
    "    \n",
    "\n",
    "    def generate_text_from_2_to_2(self,seed_text, next_words):\n",
    "        #the main workflow is twett_class_1 -> encode_class1-> z -> decode_class_2\n",
    "        model = tf.keras.models.load_model('Training_class_2.h5')\n",
    "        gen_w = \"\"\n",
    "        \n",
    "        #enconding_class1 \n",
    "        for _ in range(next_words):\n",
    "            token_list = self.tokenizer_2.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences([token_list],maxlen=self.max_sequence_len_class2-1, padding='pre')\n",
    "            #predicted = model.predict_classes(token_list, verbose=0)\n",
    "            #decode with model_class_2\n",
    "            predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "            output_word = \"\"\n",
    "            #descompress for argentina xd\n",
    "            for word,index in self.tokenizer_2.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            seed_text += \" \"+output_word\n",
    "            gen_w += \" \"+output_word\n",
    "            \n",
    "        return gen_w.title()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef57749-34fd-4701-93c3-cb94d9dad9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e692ee53-8a5f-46fb-9a15-8227ec019b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the class\n",
    "robot = robot_predictor_trainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47d5561-e662-4f7c-b34a-eeeb1ba05322",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mexico_en_.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11144/2067239644.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#load the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrobot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dataset_class_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrobot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dataset_class_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11144/820014976.py\u001b[0m in \u001b[0;36mload_dataset_class_1\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m#The methods avaialabe to Load the information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_dataset_class_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mdf_mexico\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mexico_en_.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;31m#lnguage = 'en'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m#df_mexico_1 = df_mexico.loc[df_mexico['lang']==lnguage]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mexico_en_.csv'"
     ]
    }
   ],
   "source": [
    "#load the data \n",
    "\n",
    "robot.load_dataset_class_1()\n",
    "\n",
    "robot.load_dataset_class_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b63e50-6e9d-4216-b998-543df34880db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh wow ricardo la volpe mauro camoranesi starred fun moment discussed importance seleccionmexicana reaching fifth game qatar they also sent harsh message liga mx tco gmzkdhebby', 'many fans seleccinmexicana selling tickets mexico vs saudiarabia game allaboutqatar tco icrv e oty', 'meanwhile little princesses misnamed seleccionmexicana mind photograph relatives players score goalsbut hey expected fans still receive returning worldcupmiseleccionmx tco j gkjhz x', 'send us one seleccionmexicana one femexfutac teams give one forwards tco oc anzrb', 'it seen coming didn notice tata came vacation like eriksson difference eriksson sent hell months tata living almost years seleccionmexicana mexico qatar mundial', 'canelolvarez warns messi social media kicking mexicanselection shirt sports national international canelo qatar tco ytwdityh tco itbyauw', 'after threatening messi sergio kun agero cesc fabregas respond canelo lvarez tco nppixxwyej info info deportes canelo messi copamundialfifa mundial seleccionargentina seleccionmexicana tco vcrudptl h', 'tell miss chicharito legend lead mexico failure tata yondeluisa seleccionmexicana tco qdqmp sz', 'lamediocre seleccionmexicana brazil national team already round tco rhisrakl h', 'argentine actor responds canelo lvarez defends messi trampling mexican shirt qatar ag r']\n",
      "[[456, 811], [456, 811, 1209], [456, 811, 1209, 457], [456, 811, 1209, 457, 957], [456, 811, 1209, 457, 957, 2532], [456, 811, 1209, 457, 957, 2532, 2533], [456, 811, 1209, 457, 957, 2532, 2533, 2534], [456, 811, 1209, 457, 957, 2532, 2533, 2534, 520], [456, 811, 1209, 457, 957, 2532, 2533, 2534, 520, 281], [456, 811, 1209, 457, 957, 2532, 2533, 2534, 520, 281, 2535]]\n"
     ]
    }
   ],
   "source": [
    "#create copus class 1\n",
    "robot.create_corpus_clas_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc202421-f45c-4226-a10d-e36fa253a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all calves maria becerra releases album la nena de argentina tco vpwebz', 'lauradimarco taking period argentina conquered high salary us sthe default pays lie another q macriesmufa', 'bluish edutorresr antonionelli nono hippocrita would disgusted goals argentina celebrated hypothetical goals mexicoclosing comment volpe moderately known argentine coach really feels mexican flag argentina', 'edufek joke bad taste country kingdom upside argentine case beautiful country people work', 'eldi brunodigennaro alvaritomorales football game win argentina involve issues do need boys hug', 'kaeldelaj brunoformiga so switzerland better france saudi arabia team argentina copa america', 'the problem goes result belgium argentina copa america tco odhat zrg', 'ndyws yes yes take cat matches argentina mufeti family', 'the economist also defined plan argentina move forward negotiable implies reform state lowering public spending adjustment instead falling people falls political caste', 'bayernhost i realised wanted remove want argentina win loooool']\n",
      "[[197, 3728], [197, 3728, 596], [197, 3728, 596, 3729], [197, 3728, 596, 3729, 3730], [197, 3728, 596, 3729, 3730, 3731], [197, 3728, 596, 3729, 3730, 3731, 136], [197, 3728, 596, 3729, 3730, 3731, 136, 2071], [197, 3728, 596, 3729, 3730, 3731, 136, 2071, 129], [197, 3728, 596, 3729, 3730, 3731, 136, 2071, 129, 1], [197, 3728, 596, 3729, 3730, 3731, 136, 2071, 129, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "robot.create_corpus_clas_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe16af7-edfb-48a9-9b82-cb04240e4946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cc72f20-aa9a-4bed-aef5-fa90fa377bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 36, 80)            511360    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 700)               2186800   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 700)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6392)              4480792   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,178,952\n",
      "Trainable params: 7,178,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "robot.train_class_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cd268da-c5e3-4c19-b184-4cff28a647a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 40, 80)            459280    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 700)               2186800   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 700)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5741)              4024441   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,670,521\n",
      "Trainable params: 6,670,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "robot.train_class_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ac41c-6f00-4052-9247-1c39b7c025a7",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "644a6f21-76cb-4db4-af2c-64430a915870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OH WOW! üòÖ\\n\\nRicardo La Volpe and Mauro Camora...</td>\n",
       "      <td>OH WOW Ricardo La Volpe Mauro Camoranesi starr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Many fans of the #Selecci√≥nMexicana are sellin...</td>\n",
       "      <td>Many fans Selecci√≥nMexicana selling tickets Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Meanwhile, the little princesses of the misnam...</td>\n",
       "      <td>Meanwhile little princesses misnamed Seleccion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Send us one for the #SeleccionMexicana and one...</td>\n",
       "      <td>Send us one SeleccionMexicana one FEMEXFUTAC t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>It was seen coming! Didn't you notice?\\nTata c...</td>\n",
       "      <td>It seen coming Didn notice Tata came vacation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>9882</td>\n",
       "      <td>ü§∑üèæ‚Äç‚ôÇÔ∏è no way\\n#mexicoargentina #MexicoVsArgent...</td>\n",
       "      <td>way mexicoargentina MexicoVsArgentina Qatar Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>9883</td>\n",
       "      <td>What ugly football the #SeleccionMexicana play...</td>\n",
       "      <td>What ugly football SeleccionMexicana plays wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>9884</td>\n",
       "      <td>Carlos Vela fuck your reputable mother for not...</td>\n",
       "      <td>Carlos Vela fuck reputable mother supporting m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>9885</td>\n",
       "      <td>This defeat, in the 90 minutes, is all from th...</td>\n",
       "      <td>This defeat minutes tatamartino The MexicanSel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9886</th>\n",
       "      <td>9886</td>\n",
       "      <td>Pathetic, as always üòÇ\\n#MexicoVsArgentina #Sel...</td>\n",
       "      <td>Pathetic always MexicoVsArgentina SeleccionMex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9887 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  OH WOW! üòÖ\\n\\nRicardo La Volpe and Mauro Camora...   \n",
       "1              1  Many fans of the #Selecci√≥nMexicana are sellin...   \n",
       "2              2  Meanwhile, the little princesses of the misnam...   \n",
       "3              3  Send us one for the #SeleccionMexicana and one...   \n",
       "4              4  It was seen coming! Didn't you notice?\\nTata c...   \n",
       "...          ...                                                ...   \n",
       "9882        9882  ü§∑üèæ‚Äç‚ôÇÔ∏è no way\\n#mexicoargentina #MexicoVsArgent...   \n",
       "9883        9883  What ugly football the #SeleccionMexicana play...   \n",
       "9884        9884  Carlos Vela fuck your reputable mother for not...   \n",
       "9885        9885  This defeat, in the 90 minutes, is all from th...   \n",
       "9886        9886  Pathetic, as always üòÇ\\n#MexicoVsArgentina #Sel...   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     OH WOW Ricardo La Volpe Mauro Camoranesi starr...  \n",
       "1     Many fans Selecci√≥nMexicana selling tickets Me...  \n",
       "2     Meanwhile little princesses misnamed Seleccion...  \n",
       "3     Send us one SeleccionMexicana one FEMEXFUTAC t...  \n",
       "4     It seen coming Didn notice Tata came vacation ...  \n",
       "...                                                 ...  \n",
       "9882  way mexicoargentina MexicoVsArgentina Qatar Se...  \n",
       "9883  What ugly football SeleccionMexicana plays wit...  \n",
       "9884  Carlos Vela fuck reputable mother supporting m...  \n",
       "9885  This defeat minutes tatamartino The MexicanSel...  \n",
       "9886  Pathetic always MexicoVsArgentina SeleccionMex...  \n",
       "\n",
       "[9887 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('clean_data_c_1.csv')\n",
    "#df.head()\n",
    "c_1 = list(df['clean_tweet'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b8eb2aa-d2e8-4094-9d8d-af0c939f553d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>All calves when Maria Becerra releases her alb...</td>\n",
       "      <td>All calves Maria Becerra releases album La Nen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@_Lauradimarco of those 20, taking the period ...</td>\n",
       "      <td>_Lauradimarco taking period Argentina conquere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@bluish97 @edutorresr @antonio_nelli Nono, Hip...</td>\n",
       "      <td>bluish edutorresr antonio_nelli Nono Hippocrit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@edufek that is a joke in bad taste, that this...</td>\n",
       "      <td>edufek joke bad taste country kingdom upside A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@Eldi2 @brunodigennaro9 @alvaritomorales is ju...</td>\n",
       "      <td>Eldi brunodigennaro alvaritomorales football g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4112</th>\n",
       "      <td>4112</td>\n",
       "      <td>I love you #VamosArgentina #Messi #Qatar2022 #...</td>\n",
       "      <td>I love VamosArgentina Messi Qatar Argentina Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4113</th>\n",
       "      <td>4113</td>\n",
       "      <td>#CadaD√≠aTeQuieroM√°s is a few hours away, my be...</td>\n",
       "      <td>CadaD√≠aTeQuieroM√°s hours away best plan since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>4114</td>\n",
       "      <td>#SeleccionMayor Be it a little, or a long time...</td>\n",
       "      <td>SeleccionMayor Be little long time one already...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4115</th>\n",
       "      <td>4115</td>\n",
       "      <td>https://t.co/demfW3LWWc\\nhttps://t.co/k67FzA1f...</td>\n",
       "      <td>tco demfW LWWc tco k FzA fFz tco TP DaDcnH GH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4116</th>\n",
       "      <td>4116</td>\n",
       "      <td>https://t.co/demfW3LWWc\\nhttps://t.co/k67FzA1f...</td>\n",
       "      <td>tco demfW LWWc tco k FzA fFz tco TP DaDcnH GH ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4117 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  All calves when Maria Becerra releases her alb...   \n",
       "1              1  @_Lauradimarco of those 20, taking the period ...   \n",
       "2              2  @bluish97 @edutorresr @antonio_nelli Nono, Hip...   \n",
       "3              3  @edufek that is a joke in bad taste, that this...   \n",
       "4              4  @Eldi2 @brunodigennaro9 @alvaritomorales is ju...   \n",
       "...          ...                                                ...   \n",
       "4112        4112  I love you #VamosArgentina #Messi #Qatar2022 #...   \n",
       "4113        4113  #CadaD√≠aTeQuieroM√°s is a few hours away, my be...   \n",
       "4114        4114  #SeleccionMayor Be it a little, or a long time...   \n",
       "4115        4115  https://t.co/demfW3LWWc\\nhttps://t.co/k67FzA1f...   \n",
       "4116        4116  https://t.co/demfW3LWWc\\nhttps://t.co/k67FzA1f...   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     All calves Maria Becerra releases album La Nen...  \n",
       "1     _Lauradimarco taking period Argentina conquere...  \n",
       "2     bluish edutorresr antonio_nelli Nono Hippocrit...  \n",
       "3     edufek joke bad taste country kingdom upside A...  \n",
       "4     Eldi brunodigennaro alvaritomorales football g...  \n",
       "...                                                 ...  \n",
       "4112  I love VamosArgentina Messi Qatar Argentina Se...  \n",
       "4113  CadaD√≠aTeQuieroM√°s hours away best plan since ...  \n",
       "4114  SeleccionMayor Be little long time one already...  \n",
       "4115  tco demfW LWWc tco k FzA fFz tco TP DaDcnH GH ...  \n",
       "4116  tco demfW LWWc tco k FzA fFz tco TP DaDcnH GH ...  \n",
       "\n",
       "[4117 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv('clean_data_c_2.csv')\n",
    "#df.head()\n",
    "c_2 = list(df2['clean_tweet'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08083447-ee54-42b3-910b-47348b97634c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sergio Kun Ag√ºero responds Canelo √Ålvarez challenging Leo Messi SeleccionMexicana SeleccionArgentina MexicovsArgentina ArgentinavsMexico MundialQatar MundialQatar Qatar QatarWorldCup QatarWorldCup Messi Aguero Fabregas tco MAQuzqSgzP'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_1[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "127830a9-47b0-4ea4-b04e-9b6e74179182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Gch J Uncle Q Argentina Tco Tjhsjfpgup Aenfoco Corrupcionojusticia Poverty Dollar Goodmonday Messi Argentina Enzo Bangladesh Baradel Malvinas La Plata La Plata V Laugh Going Tco Qpi Czsz H Corrupcionojusticia Poverty Dollar Buenl Bangladesh Argentina Bangladesh Malvinas La Plata H Going Uruguay Uruguay Uruguay Uruguay Uruguay Uruguay Uruguay Uruguay Uruguay Uruguay Don Flasheaste Already Already Buy Luis Juez Nocontextfooty Deal'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot.generate_text_from_2_to_2(seed_text=c_1[150],next_words=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c70063b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 289ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'w j r fact argentina elcz sw z v corrupcionojusticia poverty dollar goodmonday messi argentina enzo bangladesh baradel malvinas plata plata v they archbishop up uhpygiwgmc js corrupcionojusticia poverty dollar goodmonday messi argentina enzo bangladesh baradel malvinas plata plata h typ gvte typ gvte corrupcionojusticia poverty dollar goodmonday messi'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean2 = robot.generate_text_from_2_to_2(seed_text=c_1[900],next_words=60)\n",
    "clean_tweet(clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "186bea7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bluish edutorresr antonio_nelli Nono Hippocrita would disgusted goals Argentina celebrated hypothetical goals MexicoClosing comment volpe moderately known Argentine coach really feels Mexican flag Argentina'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e96e0c7c-8e1d-4e5f-8f20-b90df210b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 266ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' National Team And Nothing Take Tata Martino Celebrate Mexico Qatar Seleccionmexicana Tco X Myyawt Ax Bqitx Qatar Seleccionmexicana Tco Ffjyfjdkvc Agv E Bd Tco Kybkhayp X Deportrecelavozdetupasion Seleccionmexicana Tco Vwtpsx'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot.generate_text_from_1_to_1(seed_text=c_2[2],next_words=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8bb5a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean tweet text by removing links, special characters\n",
    "    using simple regex statements.\n",
    "    '''\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    '''\n",
    "    stopwords = [\"s\", \"m\", \"will\", \"co\", \"t\", \"c\", \"l\", \"un\", \"y\", \"ma\", \"la\", \"d\", \"que\", \"por\", \"el\", \"n\", \"lo\", \"para\", \"n\", \"tco\", \"vwtpsx\", \"agv\", \n",
    "    \"bd\", \"q\", \"fb\", \"vu\"]\n",
    "    temp = tweet.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    #temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in stopwords]\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fac49279-ecca-4f6c-b457-03c45530f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rc z ko qatar seleccionmexicana miseleccionmx ochoa xapifurtkq fuz nzx yondeluisa xapifurtkq cq v v seleccionmexicana mundialqatar xapifurtkq e funesmori e e e hahaha dmmzkyejoi x purpose oyj seleccionmexicana hswgsapvm x via gobiernomx tvazteca memoochoa lionelmessi snm seleccionmexicana pf f tudnusa'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xd =robot.generate_text_from_1_to_1(seed_text=c_2[800],next_words=50)\n",
    "clean_tweet(xd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdedd7-e52b-4753-9647-54a88f5fd7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bf9e1-dcdc-44da-b51f-8af161d22a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd3245-6ab1-4af2-aa27-1a4cc881b75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70b682-4f1b-40b7-bdcb-29b6abe10653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aafae7-099f-4599-a25f-aab48a664a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
