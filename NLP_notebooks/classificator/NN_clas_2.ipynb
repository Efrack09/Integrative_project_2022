{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b025146-51b8-458a-8b0a-bab17978631a",
   "metadata": {},
   "source": [
    "# Predictor Class \n",
    "\n",
    "## Introduction \n",
    "\n",
    "So far in this notebook we are going to create a predictor class for \n",
    "detectin to what class a given tweet belongs to. \n",
    "\n",
    "the class 1 is Mexico and the class 2 is argentina. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f7800b-a15b-4a50-b88a-6af48219642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class robot_class_predictor: \n",
    "    def __init__(self):\n",
    "        #here we have the global variables.so far we need \n",
    "        #1. the data frames \n",
    "        # the model architecture   \n",
    "        #Noise removal, stop word removal, normalizing?\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        self.df_class_1=None\n",
    "        self.df_class_2=None\n",
    "        self.concat_df=None\n",
    "        self.tfidf=None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def cleanString(self,s, special_chars = \"\\\":,.@|√∞√ø≈ì≈æ√∞√ø√¢≈ì≈ì√Ø√ø≈ì≈æ√ø¬∫√ø√ø≈ì≈æ√ø\"):\n",
    "        from nltk.tokenize import TweetTokenizer\n",
    "        from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "        for char in special_chars:\n",
    "            s = s.replace(char, \"\")\n",
    "        s = s.replace(\"\\n\", \"\")\n",
    "        s = s.replace(\"https\", \"\")\n",
    "        s = self.scrub_words(s)\n",
    "        tokenizer = TweetTokenizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        cleaned_words = [w for w in tokenizer.tokenize(s) if w not in stop_words]\n",
    "        return \" \".join(cleaned_words)\n",
    "    \n",
    "    def cleanFrame(self,frame):\n",
    "        frame['clean_tweet'] = frame.tweet.apply(self.cleanString)\n",
    "        #methods for cleaning the tweets\n",
    "        \n",
    "        \n",
    "    #preprocessing methods\n",
    "    def scrub_words(self,text):\n",
    "        # remove html markup\n",
    "        import re\n",
    "        text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "        #remove non-ascii and digits\n",
    "        text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "  \n",
    "        #remove whitespace\n",
    "        text=text.strip()\n",
    "        return text\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    #The methods avaialabe to Load the information  \n",
    "    def load_dataset_class_1(self):\n",
    "        df_mexico = pd.read_csv('mexico_en_.csv')\n",
    "        lnguage = 'en'\n",
    "        df_mexico_1 = df_mexico.loc[df_mexico['lang']==lnguage]        \n",
    "        t_1=pd.DataFrame({'tweet':list(df_mexico_1['text']),'class':0})\n",
    "        #load the data to the class\n",
    "        self.cleanFrame(t_1)\n",
    "        self.df_class_1 = t_1\n",
    "        t_1.to_csv('C_clean_data_c_1.csv')\n",
    "        print('Data loaded successfully')\n",
    "\n",
    "    def load_dataset_class_2(self):\n",
    "        df_argentina = pd.read_csv('argentina_en.csv')\n",
    "        lnguage = 'en'\n",
    "        df_argentina_1 = df_argentina.loc[df_argentina['lang']==lnguage]        \n",
    "        t_1=pd.DataFrame({'tweet':list(df_argentina_1['text']),'class':1})\n",
    "        #load the data to the class\n",
    "        self.cleanFrame(t_1)\n",
    "        self.df_class_2 = t_1\n",
    "        t_1.to_csv('C_clean_data_c_2.csv')\n",
    "\n",
    "        print('Data loaded successfully')\n",
    "        \n",
    "        \n",
    "    #=====================================================#\n",
    "    def tweetToVec(self,tweet, row=0):\n",
    "        words = tweet.split(\" \")\n",
    "        vec = np.zeros(self.tfidf.shape[1])\n",
    "        for w in words:\n",
    "            #print(\"including word \" + w)\n",
    "            if w in self.tfidf.columns:\n",
    "                index = self.tfidf.columns.get_loc(w)\n",
    "            #print(index, tfidf[w][row])\n",
    "            vec[index] = self.tfidf[w][row]\n",
    "        return vec\n",
    "    \n",
    "    \n",
    "    def prepare_metadata(self):\n",
    "        self.load_dataset_class_1()\n",
    "        self.load_dataset_class_2()\n",
    "        \n",
    "        self.concat_df = pd.concat([self.df_class_1,self.df_class_2])\n",
    "        \n",
    "            #from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        vectors = self.vectorizer.fit_transform(self.concat_df.tweet.tolist())\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        self.tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "        self.tfidf.head()\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # code for the training model \n",
    "    def prepare_data_for_tidf(self):\n",
    "        \n",
    "        self.load_dataset_class_1()\n",
    "        self.load_dataset_class_2()\n",
    "        \n",
    "        self.concat_df = pd.concat([self.df_class_1,self.df_class_2])\n",
    "        \n",
    "            #from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        vectors = self.vectorizer.fit_transform(self.concat_df.tweet.tolist())\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        self.tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "        self.tfidf.head()\n",
    "        \n",
    "        #print(self.tweetToVec(list(self.concat_df['tweet'])[0],self.tfidf))\n",
    "        \n",
    "        \n",
    "        X = self.tfidf.values\n",
    "        print(X.shape)\n",
    "        print(np.ones(X.shape[0]).shape)\n",
    "        #Adding bias term\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        print(X.shape)\n",
    "        \n",
    "        Y = np.array(list(map(lambda x: min(x, 1),self.concat_df['class'])))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        print(X.shape, X_train.shape, X_test.shape, len(y_train), len(y_test))\n",
    "        \n",
    "        \n",
    "        training_data = X_train\n",
    "        target_data = np.array(y_train)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(X.shape[1], activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "        model.fit(training_data, target_data, epochs=150)\n",
    " \n",
    "        scores = model.evaluate(training_data, target_data)\n",
    "\n",
    "        model.save('Training_clasification_v2.h5')\n",
    "\n",
    " \n",
    "        print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        print (model.predict(training_data).round())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ############################3\n",
    "    def predict_new(self,tweet):\n",
    "        model = tf.keras.models.load_model('Training_clasification_v2.h5')\n",
    "        #vectorize = self.tweetToVec(tweet,row=0)\n",
    "        tweet2 = [tweet,'a']\n",
    "        vectors = self.vectorizer.transform(tweet2)\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "        X=tfidf.values\n",
    "        \n",
    "        #X = vectorize\n",
    "        print(X.shape)\n",
    "        print(np.ones(X.shape[0]).shape)\n",
    "        #Adding bias term\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        print(X.shape)\n",
    "\n",
    "        val = model.predict(X).round()\n",
    "        print(model.predict(X).round())\n",
    "\n",
    "        return val\n",
    "\n",
    "            \n",
    "                                                     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b43d7e4-9f39-4c4c-9881-b095d1ed2ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\EFRACK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing.label import MultiLabelBinarizer\n",
    "from sklearn.preprocessing._label import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np \n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb8240a-e56a-492f-871b-d6d476f37d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_e = robot_class_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79fef88-629d-48d7-92a1-4d4c379a909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Data loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EFRACK\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "wall_e.prepare_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "154ee054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Data loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EFRACK\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14004, 14355)\n",
      "(14004,)\n",
      "(14004, 14356)\n",
      "(14004, 14356) (11203, 14356) (2801, 14356) 11203 2801\n",
      "Epoch 1/150\n",
      "351/351 [==============================] - 162s 456ms/step - loss: 0.1175 - accuracy: 0.9497\n",
      "Epoch 2/150\n",
      "351/351 [==============================] - 160s 457ms/step - loss: 0.0125 - accuracy: 0.9971\n",
      "Epoch 3/150\n",
      "351/351 [==============================] - 160s 455ms/step - loss: 0.0061 - accuracy: 0.9988\n",
      "Epoch 4/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0042 - accuracy: 0.9989\n",
      "Epoch 5/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 6/150\n",
      "351/351 [==============================] - 159s 453ms/step - loss: 0.0040 - accuracy: 0.9988\n",
      "Epoch 7/150\n",
      "351/351 [==============================] - 161s 458ms/step - loss: 0.0023 - accuracy: 0.9988\n",
      "Epoch 8/150\n",
      "351/351 [==============================] - 165s 471ms/step - loss: 0.0020 - accuracy: 0.9988\n",
      "Epoch 9/150\n",
      "351/351 [==============================] - 162s 462ms/step - loss: 0.0015 - accuracy: 0.9991\n",
      "Epoch 10/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0021 - accuracy: 0.9988\n",
      "Epoch 11/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0021 - accuracy: 0.9988\n",
      "Epoch 12/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0019 - accuracy: 0.9992\n",
      "Epoch 13/150\n",
      "351/351 [==============================] - 161s 459ms/step - loss: 0.0020 - accuracy: 0.9990\n",
      "Epoch 14/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0133 - accuracy: 0.9961\n",
      "Epoch 15/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0033 - accuracy: 0.9988\n",
      "Epoch 16/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0019 - accuracy: 0.9990\n",
      "Epoch 17/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0015 - accuracy: 0.9990\n",
      "Epoch 18/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0031 - accuracy: 0.9989\n",
      "Epoch 19/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0014 - accuracy: 0.9989\n",
      "Epoch 20/150\n",
      "351/351 [==============================] - 159s 453ms/step - loss: 0.0014 - accuracy: 0.9989\n",
      "Epoch 21/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 22/150\n",
      "351/351 [==============================] - 159s 453ms/step - loss: 0.0025 - accuracy: 0.9990\n",
      "Epoch 23/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0026 - accuracy: 0.9988\n",
      "Epoch 24/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9992\n",
      "Epoch 25/150\n",
      "351/351 [==============================] - 159s 453ms/step - loss: 0.0017 - accuracy: 0.9989\n",
      "Epoch 26/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0029 - accuracy: 0.9986\n",
      "Epoch 27/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0024 - accuracy: 0.9991\n",
      "Epoch 28/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0036 - accuracy: 0.9987\n",
      "Epoch 29/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 30/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0021 - accuracy: 0.9990\n",
      "Epoch 31/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0023 - accuracy: 0.9991\n",
      "Epoch 32/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0014 - accuracy: 0.9989\n",
      "Epoch 33/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 34/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9992\n",
      "Epoch 35/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 36/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0022 - accuracy: 0.9991\n",
      "Epoch 37/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0016 - accuracy: 0.9989\n",
      "Epoch 38/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0024 - accuracy: 0.9992\n",
      "Epoch 39/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0016 - accuracy: 0.9992\n",
      "Epoch 40/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0021 - accuracy: 0.9991\n",
      "Epoch 41/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0014 - accuracy: 0.9989\n",
      "Epoch 42/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0018 - accuracy: 0.9991\n",
      "Epoch 43/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0031 - accuracy: 0.9988\n",
      "Epoch 44/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 45/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 46/150\n",
      "351/351 [==============================] - 159s 453ms/step - loss: 0.0017 - accuracy: 0.9990\n",
      "Epoch 47/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 48/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 49/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 50/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 51/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0014 - accuracy: 0.9991\n",
      "Epoch 52/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 53/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0019 - accuracy: 0.9989\n",
      "Epoch 54/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0014 - accuracy: 0.9990\n",
      "Epoch 55/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 56/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0014 - accuracy: 0.9990\n",
      "Epoch 57/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 58/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 59/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 60/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 61/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 62/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0029 - accuracy: 0.9990\n",
      "Epoch 63/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0036 - accuracy: 0.9985\n",
      "Epoch 64/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0036 - accuracy: 0.9989\n",
      "Epoch 65/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0015 - accuracy: 0.9989\n",
      "Epoch 66/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 67/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 68/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0023 - accuracy: 0.9993\n",
      "Epoch 69/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0022 - accuracy: 0.9991\n",
      "Epoch 70/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0014 - accuracy: 0.9992\n",
      "Epoch 71/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0030 - accuracy: 0.9986\n",
      "Epoch 72/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0016 - accuracy: 0.9991\n",
      "Epoch 73/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0015 - accuracy: 0.9990\n",
      "Epoch 74/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0021 - accuracy: 0.9992\n",
      "Epoch 75/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0018 - accuracy: 0.9989\n",
      "Epoch 76/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0019 - accuracy: 0.9993\n",
      "Epoch 77/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0014 - accuracy: 0.9993\n",
      "Epoch 78/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0022 - accuracy: 0.9988\n",
      "Epoch 79/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0019 - accuracy: 0.9988\n",
      "Epoch 80/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0047 - accuracy: 0.9988\n",
      "Epoch 81/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0015 - accuracy: 0.9990\n",
      "Epoch 82/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 83/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0013 - accuracy: 0.9992\n",
      "Epoch 84/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 85/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 86/150\n",
      "351/351 [==============================] - 160s 455ms/step - loss: 0.0014 - accuracy: 0.9991\n",
      "Epoch 87/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0014 - accuracy: 0.9992\n",
      "Epoch 88/150\n",
      "351/351 [==============================] - 158s 452ms/step - loss: 0.0015 - accuracy: 0.9991\n",
      "Epoch 89/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0027 - accuracy: 0.9986\n",
      "Epoch 90/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 91/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 92/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0014 - accuracy: 0.9992\n",
      "Epoch 93/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0015 - accuracy: 0.9993\n",
      "Epoch 94/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0020 - accuracy: 0.9990\n",
      "Epoch 95/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0017 - accuracy: 0.9990\n",
      "Epoch 96/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 97/150\n",
      "351/351 [==============================] - 159s 454ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 98/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0024 - accuracy: 0.9989\n",
      "Epoch 99/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 100/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0017 - accuracy: 0.9989\n",
      "Epoch 101/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0014 - accuracy: 0.9989\n",
      "Epoch 102/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0014 - accuracy: 0.9991\n",
      "Epoch 103/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 104/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0013 - accuracy: 0.9992\n",
      "Epoch 105/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0035 - accuracy: 0.9989\n",
      "Epoch 106/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0017 - accuracy: 0.9991\n",
      "Epoch 107/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 108/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 109/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0023 - accuracy: 0.9989\n",
      "Epoch 110/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9992\n",
      "Epoch 111/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0019 - accuracy: 0.9989\n",
      "Epoch 112/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0013 - accuracy: 0.9992\n",
      "Epoch 113/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0014 - accuracy: 0.9993\n",
      "Epoch 114/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 115/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0017 - accuracy: 0.9991\n",
      "Epoch 116/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0014 - accuracy: 0.9992\n",
      "Epoch 117/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 118/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0023 - accuracy: 0.9988\n",
      "Epoch 119/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0017 - accuracy: 0.9989\n",
      "Epoch 120/150\n",
      "351/351 [==============================] - 159s 453ms/step - loss: 0.0017 - accuracy: 0.9989\n",
      "Epoch 121/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 122/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0013 - accuracy: 0.9992\n",
      "Epoch 123/150\n",
      "351/351 [==============================] - 159s 452ms/step - loss: 0.0012 - accuracy: 0.9993\n",
      "Epoch 124/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0023 - accuracy: 0.9989\n",
      "Epoch 125/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0015 - accuracy: 0.9992\n",
      "Epoch 126/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0015 - accuracy: 0.9989\n",
      "Epoch 127/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 128/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0016 - accuracy: 0.9991\n",
      "Epoch 129/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0012 - accuracy: 0.9991\n",
      "Epoch 130/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0018 - accuracy: 0.9991\n",
      "Epoch 131/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 132/150\n",
      "351/351 [==============================] - 158s 451ms/step - loss: 0.0014 - accuracy: 0.9990\n",
      "Epoch 133/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 134/150\n",
      "351/351 [==============================] - 158s 449ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 135/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 136/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 137/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 138/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0017 - accuracy: 0.9989\n",
      "Epoch 139/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0021 - accuracy: 0.9993\n",
      "Epoch 140/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0014 - accuracy: 0.9990\n",
      "Epoch 141/150\n",
      "351/351 [==============================] - 158s 449ms/step - loss: 0.0016 - accuracy: 0.9991\n",
      "Epoch 142/150\n",
      "351/351 [==============================] - 158s 449ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 143/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0014 - accuracy: 0.9989\n",
      "Epoch 144/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 145/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0013 - accuracy: 0.9990\n",
      "Epoch 146/150\n",
      "351/351 [==============================] - 158s 449ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 147/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0015 - accuracy: 0.9990\n",
      "Epoch 148/150\n",
      "351/351 [==============================] - 158s 449ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "Epoch 149/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0015 - accuracy: 0.9991\n",
      "Epoch 150/150\n",
      "351/351 [==============================] - 158s 450ms/step - loss: 0.0015 - accuracy: 0.9989\n",
      "351/351 [==============================] - 17s 49ms/step - loss: 0.0013 - accuracy: 0.9991\n",
      "\n",
      "accuracy: 99.91%\n",
      "351/351 [==============================] - 17s 49ms/step\n",
      "[[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "wall_e.prepare_data_for_tidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca71ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c7317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8a468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f138931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8a14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ac4988-b345-46ae-a671-3d3ac5c86ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OH WOW! üòÖ\\n\\nRicardo La Volpe and Mauro Camora...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Many fans of the #Selecci√≥nMexicana are sellin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meanwhile, the little princesses of the misnam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Send us one for the #SeleccionMexicana and one...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was seen coming! Didn't you notice?\\nTata c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>ü§∑üèæ‚Äç‚ôÇÔ∏è no way\\n#mexicoargentina #MexicoVsArgent...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>What ugly football the #SeleccionMexicana play...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>Carlos Vela fuck your reputable mother for not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>This defeat, in the 90 minutes, is all from th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9886</th>\n",
       "      <td>Pathetic, as always üòÇ\\n#MexicoVsArgentina #Sel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9887 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  senti\n",
       "0     OH WOW! üòÖ\\n\\nRicardo La Volpe and Mauro Camora...      0\n",
       "1     Many fans of the #Selecci√≥nMexicana are sellin...      0\n",
       "2     Meanwhile, the little princesses of the misnam...      0\n",
       "3     Send us one for the #SeleccionMexicana and one...      0\n",
       "4     It was seen coming! Didn't you notice?\\nTata c...      0\n",
       "...                                                 ...    ...\n",
       "9882  ü§∑üèæ‚Äç‚ôÇÔ∏è no way\\n#mexicoargentina #MexicoVsArgent...      0\n",
       "9883  What ugly football the #SeleccionMexicana play...      0\n",
       "9884  Carlos Vela fuck your reputable mother for not...      0\n",
       "9885  This defeat, in the 90 minutes, is all from th...      0\n",
       "9886  Pathetic, as always üòÇ\\n#MexicoVsArgentina #Sel...      0\n",
       "\n",
       "[9887 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mexico = pd.read_csv('mexico_en_.csv')\n",
    "df_mexico.head()\n",
    "#df_mexico.loc[df_mexico['lang']=='en']\n",
    "lnguage = 'en'\n",
    "# saving the data for mexico \n",
    "t_1=pd.DataFrame({'tweet':list(df_mexico.loc[df_mexico['lang']==lnguage]['text']),'senti':0})\n",
    "#t_1 = df_mexico['text']\n",
    "#t_1['class'] = 0\n",
    "t_1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3eb8cc-8964-444b-8a0a-3653406c49bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a286f16-5341-4660-b15f-9825583b0759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lamediocre #SeleccionMexicana to the #Brazil National Team already in the round of 16. https://t.co/rhisRaKL2h'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t_1['tweet'])[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129b1550-4dda-4c7e-9d46-adf48fb74300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EFRACK\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 14355)\n",
      "(2,)\n",
      "(2, 14356)\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[[0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ect = wall_e.predict_new(list(t_1['tweet'])[2])\n",
    "ect[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171f2bb9-f6b6-4315-8632-535846cc032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cbf01-d9d3-44a7-8d43-da8477f00508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41083559-8a14-4385-9874-fbc94789cf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b60b2-b74a-449e-b054-587d1a2fd1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
